{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import string\n",
    "import nltk\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tag.stanford import StanfordPOSTagger\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "FNAME = 'project_files/training.json' # Need to change later\n",
    "\n",
    "TESTNAME = 'project_files/testing.json'\n",
    "\n",
    "Q_WORDS = ['how','what','whom','when','who','where','which','name']\n",
    "SELECTED_Q = ['what','which']\n",
    "\n",
    "ner_dir = 'stanford-ner-2018-02-27/'\n",
    "ner_jarfile = ner_dir + 'stanford-ner.jar'\n",
    "ner_modelfile = ner_dir + 'classifiers/english.all.3class.distsim.crf.ser.gz'\n",
    "ner_tagger = StanfordNERTagger(model_filename=ner_modelfile, path_to_jar=ner_jarfile)\n",
    "\n",
    "pos_dir = 'stanford-postagger-2018-02-27/'\n",
    "pos_modelfile = pos_dir + 'models/english-bidirectional-distsim.tagger'\n",
    "pos_jarfile = pos_dir + 'stanford-postagger.jar'\n",
    "pos_tagger = StanfordPOSTagger(model_filename=pos_modelfile, path_to_jar=pos_jarfile)\n",
    "\n",
    "with open(FNAME) as train_data:\n",
    "    trainfile = json.load(train_data)\n",
    "\n",
    "with open(TESTNAME) as test_data:\n",
    "    testfile = json.load(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Select the required Features for multi-layer perceptron (MLP) algorithm in scikit learn (deep learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_features(trainfile):\n",
    "    feature_count = defaultdict(int)\n",
    "\n",
    "    for dic in trainfile:  \n",
    "        question = dic['question']\n",
    "        question_token = nltk.word_tokenize(question)\n",
    "        question_token = [word.lower() for word in question_token]\n",
    "        for q in SELECTED_Q:\n",
    "            if q in question_token:\n",
    "                index = question_token.index(q)\n",
    "                if index+1 < len(question_token):\n",
    "                    next_ = question_token[index + 1]\n",
    "                    feature_count[next_] += 1\n",
    "    return feature_count\n",
    "\n",
    "dic = get_features(trainfile)\n",
    "feature_count = sorted(dic.items(), key=lambda x: x[1],reverse=True)\n",
    "\n",
    "selected_feature = []\n",
    "# Only select the words whose frequencies larger than one\n",
    "for item in feature_count:\n",
    "    if item[1] > 1:\n",
    "        selected_feature.append(item[0])\n",
    "print selected_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Select the expected classes for MLP classification in scikit learn\n",
    "array y of size (n_samples,), which holds the target values (class labels) for the training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('mlp_files/all_ans_pos.json') as f:\n",
    "    lst_la = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43379"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lst_la)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_classes(trainfile):\n",
    "    all_ans = []\n",
    "    \n",
    "    with open('mlp_files/tagged_ans.json') as f:\n",
    "        tagged_ans = json.load(f)\n",
    "    \n",
    "    \n",
    "    for i in range(len(trainfile)):\n",
    "        print i\n",
    "        dic = trainfile[i]\n",
    "        ans = dic['text']\n",
    "        ans_token = nltk.word_tokenize(ans)\n",
    "    \n",
    "    \n",
    "        ans_tags = [[],[]]\n",
    "        if tagged_ans.has_key(ans):\n",
    "            ans_tags = [tagged_ans[ans][0], tagged_ans[ans][1]]\n",
    "        else:\n",
    "            pos = pos_tagger.tag(ans_token)\n",
    "\n",
    "            pos_tags = set()\n",
    "\n",
    "            for item in pos:\n",
    "                pos_tags.add(item[1])\n",
    "\n",
    "            ans_tags[1] = list(pos_tags)\n",
    "            \n",
    "            tagged_ans[ans] = ans_tags\n",
    "\n",
    "        all_ans.append(ans_tags)\n",
    "        \n",
    "    with open('mlp_files/tagged_ans.json', 'w') as f:\n",
    "        json.dump(tagged_ans, f)\n",
    "        \n",
    "    return all_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# def get_classes(trainfile):\n",
    "#     all_ans = []\n",
    "    \n",
    "#     with open('mlp_files/tagged_ans.json') as f:\n",
    "#         tagged_ans = json.load(f)\n",
    "    \n",
    "    \n",
    "#     for dic in trainfile:\n",
    "#         ans = dic['text']\n",
    "#         ans_token = nltk.word_tokenize(ans)\n",
    "        \n",
    "#         if tagged_ans.has_key(ans):\n",
    "#             ans_tags = [tagged_ans[ans][0], tagged_ans[ans][1]]\n",
    "#         else:\n",
    "#             ner = ner_tagger.tag(ans_token)\n",
    "#             pos = pos_tagger.tag(ans_token)\n",
    "\n",
    "#             ner_tags = set()\n",
    "#             pos_tags = set()\n",
    "\n",
    "#             for item in ner:\n",
    "#                 ner_tags.add(item[1])\n",
    "            \n",
    "# #             print ner_tags\n",
    "            \n",
    "#             if 'O' in ner_tags:\n",
    "#                 ner_tags.remove('O')\n",
    "\n",
    "#             for item in pos:\n",
    "#                 pos_tags.add(item[1])\n",
    "\n",
    "#             ans_tags = [list(ner_tags), list(pos_tags)]\n",
    "            \n",
    "#             tagged_ans[ans] = ans_tags\n",
    "            \n",
    "#             with open('mlp_files/tagged_ans.json', 'w') as f:\n",
    "#                 json.dump(tagged_ans, f)\n",
    "\n",
    "#         all_ans.append(ans_tags)\n",
    "        \n",
    "#     return all_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create feature vectors of training samples \n",
    "array X of size (n_samples, n_features), which holds the training samples represented as floating point feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_vectors(infile, vec_file_name):\n",
    "    all_vec = []\n",
    "\n",
    "    q_word_len = len(Q_WORDS)\n",
    "    feature_len = len(selected_feature)\n",
    "    \n",
    "    with open(vec_file_name) as vec_f:\n",
    "        vecfile = json.load(vec_f)\n",
    "    \n",
    "    for dic in infile: \n",
    "        \n",
    "        ques = dic['question']\n",
    "        \n",
    "        if vecfile.has_key(ques):\n",
    "            vec = vecfile[ques]\n",
    "        else:\n",
    "            vec = []\n",
    "            vec += [0] * (q_word_len + feature_len)\n",
    "\n",
    "            ques_token = nltk.word_tokenize(ques)\n",
    "            ques_token = [word.lower() for word in ques_token]\n",
    "\n",
    "            for i in range(q_word_len):\n",
    "                if Q_WORDS[i] in ques_token:\n",
    "                    vec[i] = 1\n",
    "\n",
    "            for j in range(feature_len):\n",
    "                if selected_feature[j] in ques_token:\n",
    "                    vec[q_word_len+j] = 1\n",
    "            \n",
    "            vecfile[ques] = vec\n",
    "                \n",
    "        all_vec.append(vec)\n",
    "    \n",
    "    with open(vec_file_name, 'w') as vec_f:\n",
    "        json.dump(vecfile, vec_f)\n",
    "    \n",
    "    return all_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train = get_vectors(trainfile, 'mlp_files/train_vec.json')\n",
    "print \"X_train\"\n",
    "\n",
    "X_test = get_vectors(testfile, 'mlp_files/test_vec.json')\n",
    "print \"X_test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# y_train_all_ans is [[NER tags], [POS tags]]\n",
    "# will be converted to two 1-d array of integers later\n",
    "y_train_all_ans = get_classes(trainfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open('mlp_files/y_train_all_ans.json', 'w') as f:\n",
    "    json.dump(y_train_all_ans, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# check all items in y_train_all_ans are list type\n",
    "# for item in y_train_all_ans:\n",
    "#     for item1 in item:\n",
    "#         if type(item1) != list:\n",
    "#             print item1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_tags_index(tags, all_ans_type):\n",
    "    if tags in all_ans_type:\n",
    "        tags_index = all_ans_type.index(tags)\n",
    "    else:\n",
    "        all_ans_type.append(tags)\n",
    "        tags_index = len(all_ans_type) - 1\n",
    "    \n",
    "    return tags_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print y_train_all_ans[40000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "all_ans_ner_type = []\n",
    "all_ans_pos_type = []\n",
    "\n",
    "all_ans_ner = []\n",
    "all_ans_pos = []\n",
    "\n",
    "for item in y_train_all_ans:\n",
    "    ner_tags = set(item[0])\n",
    "    pos_wanted = item[1]\n",
    "\n",
    "    while type(pos_wanted[0]) == list:\n",
    "        pos_wanted = pos_wanted[0]\n",
    "\n",
    "    pos_tags = set(pos_wanted)\n",
    "    \n",
    "    all_ans_ner.append(get_tags_index(ner_tags, all_ans_ner_type))\n",
    "    all_ans_pos.append(get_tags_index(pos_tags, all_ans_pos_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open('mlp_files/all_ans_ner.json', 'w') as f:\n",
    "    json.dump(all_ans_ner, f)\n",
    "    \n",
    "with open('mlp_files/all_ans_pos.json', 'w') as f:\n",
    "    json.dump(all_ans_pos, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "# Fit only to the training data\n",
    "scaler.fit(X_train)\n",
    "# Now apply the transformations to the data:\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# mlp_ner = MLPClassifier(hidden_layer_sizes=(13,13,13),max_iter=500)\n",
    "# mlp_ner.fit(X_train,all_ans_ner)\n",
    "\n",
    "# mlp_pos = MLPClassifier(hidden_layer_sizes=(13,13,13),max_iter=500)\n",
    "# mlp_pos.fit(X_train,all_ans_pos)\n",
    "\n",
    "mlp_ner = MLPClassifier(hidden_layer_sizes=(100, 100))\n",
    "mlp_ner.fit(X_train,all_ans_ner)\n",
    "\n",
    "mlp_pos = MLPClassifier(hidden_layer_sizes=(100, 100))\n",
    "mlp_pos.fit(X_train,all_ans_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "mlp_ner_pred = mlp_ner.predict(X_test)\n",
    "mlp_pos_pred = mlp_pos.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_tags_from_index(pred, all_ans_type):\n",
    "    \n",
    "    pred_tags = []\n",
    "    \n",
    "    for item in pred:\n",
    "        pred_tags.append(list(all_ans_type[item]))\n",
    "    \n",
    "    return pred_tags\n",
    "    \n",
    "mlp_ner_pred_tags = get_tags_from_index(mlp_ner_pred, all_ans_ner_type)\n",
    "mlp_pos_pred_tags = get_tags_from_index(mlp_pos_pred, all_ans_pos_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open('mlp_files/mlp_ner_pred_tags.json', 'w') as f:\n",
    "    json.dump(mlp_ner_pred_tags, f)\n",
    "    \n",
    "with open('mlp_files/mlp_pos_pred_tags.json', 'w') as f:\n",
    "    json.dump(mlp_pos_pred_tags, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
