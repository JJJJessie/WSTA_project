{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import string\n",
    "import nltk\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "FNAME = 'project_files/devel.json' # Need to change later\n",
    "\n",
    "TESTNAME = 'project_files/testing.json'\n",
    "\n",
    "Q_WORDS = ['how','what','whom','when','who','where','which']\n",
    "SELECTED_Q = ['how','what','which']\n",
    "\n",
    "ner_dir = 'stanford-ner-2018-02-27/'\n",
    "ner_jarfile = ner_dir + 'stanford-ner.jar'\n",
    "ner_modelfile = ner_dir + 'classifiers/english.all.3class.distsim.crf.ser.gz'\n",
    "ner_tagger = StanfordNERTagger(model_filename=ner_modelfile, path_to_jar=ner_jarfile)\n",
    "\n",
    "pos_dir = 'stanford-postagger-2018-02-27/'\n",
    "pos_modelfile = pos_dir + 'models/english-bidirectional-distsim.tagger'\n",
    "pos_jarfile = pos_dir + 'stanford-postagger.jar'\n",
    "pos_tagger = StanfordPOSTagger(model_filename=pos_modelfile, path_to_jar=pos_jarfile)\n",
    "\n",
    "with open(FNAME) as json_data:\n",
    "    infile = json.load(json_data)\n",
    "\n",
    "with open(TESTNAME) as test_data:\n",
    "    testfile = json.load(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the required Features for multi-layer perceptron (MLP) algorithm in scikit learn (deep learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'is', u'many', u'was', u'year', u'did', u'type', u'does', u'country', u'percentage', u'much', u'kind', u'?', u'language', u'other', u'date', u'city', u\"'s\", u'do', u'political', u'are', u'century', u'long', u'company', u'month', u'can', u'two', u'group', u'were', u'period', u'percent', u'sort', u'word', u'part', u'would', u'decade', u'term', u'notable', u'age', u'has', u'of', u'form', u'neighborhood', u'organization', u'color', u'old', u'branch', u'far', u'award', u'dutch', u'isotope', u'else', u'industry', u'area', u'era', u'nation', u'event', u'state', u'party', u'caused', u'large', u'town', u'dialect', u'philosophy', u'types', u'years', u'germanic', u'name', u'university', u'temperature', u'place', u'number', u'rank', u'individual', u'nationality', u'region', u'book', u'religion', u'magazine', u'tribe', u'span', u'empire', u'time', u'concept', u'dynasty', u'gender', u'street', u'action', u'family', u'county', u'must', u'work', u'topic', u'element', u'geographic', u'material', u'could', u'major', u'quality', u'province', u'pope', u'german', u'title', u'areas', u'gospel', u'computer', u'calendar', u'became', u'character', u'community', u'geographical', u'aspect', u'console', u'world', u'often', u'people', u'church', u'business', u'ideology', u'sound', u'egyptian', u'second', u'section', u'version', u'military', u'army', u'native', u'court', u'divides', u'ethnic', u'president', u'man', u'light', u'interrupted', u'style', u'theologian', u'happened', u'day', u'series', u'principles', u'dialects', u'revolutionary', u'florida', u'publication', u'system', u'relationship', u'park', u'institution', u'angel', u'modern', u'subjects', u'class', u'professional', u'factor', u'coast', u'famous', u'countries', u'river', u'rmp', u'national', u'culture', u'subject', u'genre', u'cities', u'point', u'direction', u'child', u'former', u'will', u'hayek', u'parts', u'ocean', u'person', u'position', u'the', u'bodies', u'had', u'lake', u'government', u'ruler', u'economic', u'philosopher', u'trophy', u'doctrine', u'fraction', u'war', u'with', u'tournament', u'film', u'building']\n"
     ]
    }
   ],
   "source": [
    "def get_features(infile):\n",
    "    feature_count = defaultdict(int)\n",
    "\n",
    "    for dic in infile:  \n",
    "        question = dic['question']\n",
    "        question_token = nltk.word_tokenize(question)\n",
    "        question_token = [word.lower() for word in question_token]\n",
    "        for q in SELECTED_Q:\n",
    "            if q in question_token:\n",
    "                index = question_token.index(q)\n",
    "                next_ = question_token[index + 1]\n",
    "                feature_count[next_] += 1\n",
    "    return feature_count\n",
    "\n",
    "dic = get_features(infile)\n",
    "feature_count = sorted(dic.items(), key=lambda x: x[1],reverse=True)\n",
    "\n",
    "selected_feature = []\n",
    "# Only select the words whose frequencies larger than one\n",
    "for item in feature_count:\n",
    "    if item[1] > 1:\n",
    "        selected_feature.append(item[0])\n",
    "print selected_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the expected classes for MLP classification in scikit learn\n",
    "array y of size (n_samples,), which holds the target values (class labels) for the training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_classes(infile):\n",
    "    all_ans = []\n",
    "    \n",
    "    for dic in infile:\n",
    "        ans = dic['text']\n",
    "        ans_token = nltk.word_tokenize(ans)\n",
    "        ner = ner_tagger.tag(ans_token)\n",
    "        pos = pos_tagger.tag(ans_token)\n",
    "        \n",
    "        ner_tags = set()\n",
    "        pos_tags = set()\n",
    "        \n",
    "        for item in ner:\n",
    "            ner_tags.add(item[1])\n",
    "        \n",
    "        if 'O' in ner_tags:\n",
    "            ner_tags.remove('O')\n",
    "        \n",
    "        for item in pos:\n",
    "            pos_tags.add(item[1])\n",
    "        \n",
    "        ans_tags = [ner_tags, pos_tags]\n",
    "        \n",
    "        all_ans.append(ans_tags)\n",
    "        \n",
    "    return all_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create feature vectors of training samples \n",
    "array X of size (n_samples, n_features), which holds the training samples represented as floating point feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_vectors(infile):\n",
    "    all_vec = []\n",
    "\n",
    "    q_word_len = len(Q_WORDS)\n",
    "    feature_len = len(selected_feature)\n",
    "    \n",
    "    for dic in infile: \n",
    "        vec = []\n",
    "        vec += [0] * (q_word_len + feature_len)\n",
    "        \n",
    "        ques = dic['question']\n",
    "        ques_token = nltk.word_tokenize(ques)\n",
    "        ques_token = [word.lower() for word in ques_token]\n",
    "        \n",
    "        for i in range(q_word_len):\n",
    "            if Q_WORDS[i] in ques_token:\n",
    "                vec[i] = 1\n",
    "                \n",
    "        for j in range(feature_len):\n",
    "            if selected_feature[j] in ques_token:\n",
    "                vec[q_word_len+j] = 1\n",
    "                \n",
    "        all_vec.append(vec)\n",
    "    \n",
    "    return all_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = get_vectors(infile)\n",
    "y_train = get_classes(infile)\n",
    "\n",
    "X_test = get_vectors(testfile)\n",
    "\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "# Fit only to the training data\n",
    "scaler.fit(X_train)\n",
    "# Now apply the transformations to the data:\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
