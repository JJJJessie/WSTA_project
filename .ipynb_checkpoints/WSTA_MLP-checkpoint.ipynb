{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import string\n",
    "import nltk\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tag.stanford import StanfordPOSTagger\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "FNAME = 'project_files/devel.json' # Need to change later\n",
    "\n",
    "TESTNAME = 'project_files/testing.json'\n",
    "\n",
    "Q_WORDS = ['how','what','whom','when','who','where','which']\n",
    "SELECTED_Q = ['how','what','which']\n",
    "\n",
    "ner_dir = 'stanford-ner-2018-02-27/'\n",
    "ner_jarfile = ner_dir + 'stanford-ner.jar'\n",
    "ner_modelfile = ner_dir + 'classifiers/english.all.3class.distsim.crf.ser.gz'\n",
    "ner_tagger = StanfordNERTagger(model_filename=ner_modelfile, path_to_jar=ner_jarfile)\n",
    "\n",
    "pos_dir = 'stanford-postagger-2018-02-27/'\n",
    "pos_modelfile = pos_dir + 'models/english-bidirectional-distsim.tagger'\n",
    "pos_jarfile = pos_dir + 'stanford-postagger.jar'\n",
    "pos_tagger = StanfordPOSTagger(model_filename=pos_modelfile, path_to_jar=pos_jarfile)\n",
    "\n",
    "with open(FNAME) as train_data:\n",
    "    trainfile = json.load(train_data)\n",
    "\n",
    "with open(TESTNAME) as test_data:\n",
    "    testfile = json.load(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Select the required Features for multi-layer perceptron (MLP) algorithm in scikit learn (deep learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'is', u'many', u'was', u'year', u'did', u'type', u'does', u'country', u'percentage', u'much', u'kind', u'?', u'language', u'other', u'date', u'city', u\"'s\", u'do', u'political', u'are', u'century', u'long', u'company', u'month', u'can', u'two', u'group', u'were', u'period', u'percent', u'sort', u'word', u'part', u'would', u'decade', u'term', u'notable', u'age', u'has', u'of', u'form', u'neighborhood', u'organization', u'color', u'old', u'branch', u'far', u'award', u'dutch', u'isotope', u'else', u'industry', u'area', u'era', u'nation', u'event', u'state', u'party', u'caused', u'large', u'town', u'dialect', u'philosophy', u'types', u'years', u'germanic', u'name', u'university', u'temperature', u'place', u'number', u'rank', u'individual', u'nationality', u'region', u'book', u'religion', u'magazine', u'tribe', u'span', u'empire', u'time', u'concept', u'dynasty', u'gender', u'street', u'action', u'family', u'county', u'must', u'work', u'topic', u'element', u'geographic', u'material', u'could', u'major', u'quality', u'province', u'pope', u'german', u'title', u'areas', u'gospel', u'computer', u'calendar', u'became', u'character', u'community', u'geographical', u'aspect', u'console', u'world', u'often', u'people', u'church', u'business', u'ideology', u'sound', u'egyptian', u'second', u'section', u'version', u'military', u'army', u'native', u'court', u'divides', u'ethnic', u'president', u'man', u'light', u'interrupted', u'style', u'theologian', u'happened', u'day', u'series', u'principles', u'dialects', u'revolutionary', u'florida', u'publication', u'system', u'relationship', u'park', u'institution', u'angel', u'modern', u'subjects', u'class', u'professional', u'factor', u'coast', u'famous', u'countries', u'river', u'rmp', u'national', u'culture', u'subject', u'genre', u'cities', u'point', u'direction', u'child', u'former', u'will', u'hayek', u'parts', u'ocean', u'person', u'position', u'the', u'bodies', u'had', u'lake', u'government', u'ruler', u'economic', u'philosopher', u'trophy', u'doctrine', u'fraction', u'war', u'with', u'tournament', u'film', u'building']\n"
     ]
    }
   ],
   "source": [
    "def get_features(trainfile):\n",
    "    feature_count = defaultdict(int)\n",
    "\n",
    "    for dic in trainfile:  \n",
    "        question = dic['question']\n",
    "        question_token = nltk.word_tokenize(question)\n",
    "        question_token = [word.lower() for word in question_token]\n",
    "        for q in SELECTED_Q:\n",
    "            if q in question_token:\n",
    "                index = question_token.index(q)\n",
    "                next_ = question_token[index + 1]\n",
    "                feature_count[next_] += 1\n",
    "    return feature_count\n",
    "\n",
    "dic = get_features(trainfile)\n",
    "feature_count = sorted(dic.items(), key=lambda x: x[1],reverse=True)\n",
    "\n",
    "selected_feature = []\n",
    "# Only select the words whose frequencies larger than one\n",
    "for item in feature_count:\n",
    "    if item[1] > 1:\n",
    "        selected_feature.append(item[0])\n",
    "print selected_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Select the expected classes for MLP classification in scikit learn\n",
    "array y of size (n_samples,), which holds the target values (class labels) for the training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_classes(trainfile):\n",
    "    all_ans = []\n",
    "    \n",
    "    with open('project_files/tagged_ans.json') as f:\n",
    "        tagged_ans = json.load(f)\n",
    "    \n",
    "    \n",
    "    for dic in trainfile:\n",
    "        ans = dic['text']\n",
    "        ans_token = nltk.word_tokenize(ans)\n",
    "        \n",
    "        if tagged_ans.has_key(ans):\n",
    "            ans_tags = [tagged_ans[ans][0], tagged_ans[ans][1]]\n",
    "        else:\n",
    "            ner = ner_tagger.tag(ans_token)\n",
    "            pos = pos_tagger.tag(ans_token)\n",
    "\n",
    "            ner_tags = set()\n",
    "            pos_tags = set()\n",
    "\n",
    "            for item in ner:\n",
    "                ner_tags.add(item[1])\n",
    "            \n",
    "#             print ner_tags\n",
    "            \n",
    "            if 'O' in ner_tags:\n",
    "                ner_tags.remove('O')\n",
    "\n",
    "            for item in pos:\n",
    "                pos_tags.add(item[1])\n",
    "\n",
    "            ans_tags = [list(ner_tags), list(pos_tags)]\n",
    "            \n",
    "            tagged_ans[ans] = ans_tags\n",
    "            \n",
    "            with open('project_files/tagged_ans.json', 'w') as f:\n",
    "                json.dump(tagged_ans, f)\n",
    "\n",
    "        all_ans.append(ans_tags)\n",
    "        \n",
    "    return all_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create feature vectors of training samples \n",
    "array X of size (n_samples, n_features), which holds the training samples represented as floating point feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_vectors(infile, vec_file_name):\n",
    "    all_vec = []\n",
    "\n",
    "    q_word_len = len(Q_WORDS)\n",
    "    feature_len = len(selected_feature)\n",
    "    \n",
    "    with open(vec_file_name) as vec_f:\n",
    "        vecfile = json.load(vec_f)\n",
    "    \n",
    "    for dic in infile: \n",
    "        \n",
    "        ques = dic['question']\n",
    "        \n",
    "        if vecfile.has_key(ques):\n",
    "            vec = vecfile[ques]\n",
    "        else:\n",
    "            vec = []\n",
    "            vec += [0] * (q_word_len + feature_len)\n",
    "\n",
    "            ques_token = nltk.word_tokenize(ques)\n",
    "            ques_token = [word.lower() for word in ques_token]\n",
    "\n",
    "            for i in range(q_word_len):\n",
    "                if Q_WORDS[i] in ques_token:\n",
    "                    vec[i] = 1\n",
    "\n",
    "            for j in range(feature_len):\n",
    "                if selected_feature[j] in ques_token:\n",
    "                    vec[q_word_len+j] = 1\n",
    "            \n",
    "            vecfile[ques] = vec\n",
    "                \n",
    "        all_vec.append(vec)\n",
    "    \n",
    "    with open(vec_file_name, 'w') as vec_f:\n",
    "        json.dump(vecfile, vec_f)\n",
    "    \n",
    "    return all_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train\n",
      "X_test\n"
     ]
    }
   ],
   "source": [
    "X_train = get_vectors(trainfile, 'project_files/train_vec.json')\n",
    "print \"X_train\"\n",
    "\n",
    "X_test = get_vectors(testfile, 'project_files/test_vec.json')\n",
    "print \"X_test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# y_train_all_ans is [[NER tags], [POS tags]]\n",
    "# will be converted to two 1-d array of integers later\n",
    "y_train_all_ans = get_classes(trainfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('project_files/y_train_all_ans.json', 'w') as f:\n",
    "    json.dump(y_train_all_ans, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check all items in y_train_all_ans are list type\n",
    "# for item in y_train_all_ans:\n",
    "#     for item1 in item:\n",
    "#         if type(item1) != list:\n",
    "#             print item1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tags_index(tags, all_ans_type):\n",
    "    if tags in all_ans_type:\n",
    "        tags_index = all_ans_type.index(tags)\n",
    "    else:\n",
    "        all_ans_type.append(tags)\n",
    "        tags_index = len(all_ans_type) - 1\n",
    "    \n",
    "    return tags_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_ans_ner_type = []\n",
    "all_ans_pos_type = []\n",
    "\n",
    "all_ans_ner = []\n",
    "all_ans_pos = []\n",
    "\n",
    "for item in y_train_all_ans:\n",
    "    ner_tags = set(item[0])\n",
    "    pos_tags = set(item[1])\n",
    "    \n",
    "    all_ans_ner.append(get_tags_index(ner_tags, all_ans_ner_type))\n",
    "    all_ans_pos.append(get_tags_index(pos_tags, all_ans_pos_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('project_files/all_ans_ner.json', 'w') as f:\n",
    "    json.dump(all_ans_ner, f)\n",
    "    \n",
    "with open('project_files/all_ans_pos.json', 'w') as f:\n",
    "    json.dump(all_ans_pos, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "# Fit only to the training data\n",
    "scaler.fit(X_train)\n",
    "# Now apply the transformations to the data:\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(13, 13, 13), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=500, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_ner = MLPClassifier(hidden_layer_sizes=(13,13,13),max_iter=500)\n",
    "mlp_ner.fit(X_train,all_ans_ner)\n",
    "\n",
    "mlp_pos = MLPClassifier(hidden_layer_sizes=(13,13,13),max_iter=500)\n",
    "mlp_pos.fit(X_train,all_ans_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "testing_ner_pred = mlp_ner.predict(X_test)\n",
    "testing_pos_pred = mlp_pos.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tags_from_index(pred, all_ans_type):\n",
    "    \n",
    "    pred_tags = []\n",
    "    \n",
    "    for item in pred:\n",
    "        pred_tags.append(list(all_ans_type[item]))\n",
    "    \n",
    "    return pred_tags\n",
    "    \n",
    "testing_ner_pred_tags = get_tags_from_index(testing_ner_pred, all_ans_ner_type)\n",
    "testing_pos_pred_tags = get_tags_from_index(testing_pos_pred, all_ans_pos_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('project_files/testing_ner_pred_tags.json', 'w') as f:\n",
    "    json.dump(testing_ner_pred_tags, f)\n",
    "    \n",
    "with open('project_files/testing_pos_pred_tags.json', 'w') as f:\n",
    "    json.dump(testing_pos_pred_tags, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
