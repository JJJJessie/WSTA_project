{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Team Name:     XiaoJiLingGui\n",
    "\n",
    "Kaggle Login:  siqiguo\n",
    "\n",
    "Student Name:  Siqi Guo\n",
    "\n",
    "Student ID:    743053\n",
    "\n",
    "Kaggle Login: yifan66\n",
    "\n",
    "Student Name: Yifan Wang\n",
    "\n",
    "Student ID: 784386\n",
    "\n",
    "Python version used: 2.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import requests, tarfile\n",
    "import json, string\n",
    "from math import log\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tag.stanford import StanfordPOSTagger\n",
    "\n",
    "stopwords = set(nltk.corpus.stopwords.words('english')) # wrap in a set() (see below)\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer() \n",
    "sent_segmenter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "DOCFNAME = 'project_files/documents.json'\n",
    "\n",
    "WHAT = 'what'\n",
    "WHO = 'who'\n",
    "HOW = 'how'\n",
    "WHERE = 'where'\n",
    "WHEN = 'when'\n",
    "WHICH = 'which'\n",
    "NAME = 'name'\n",
    "WHY = 'why'\n",
    "WHOM = 'whom'\n",
    "BINARY = 'binary'\n",
    "POLAR = 'polar'\n",
    "\n",
    "NER = 0\n",
    "\n",
    "POS = 1\n",
    "\n",
    "HOW_SUBCLASS = {'many': [[], ['CD']], \n",
    "                'long': [['DURATION'], ['CD']], \n",
    "                'much': [['MONEY'], ['CD', 'FW', 'NN']], \n",
    "                'far': [[], ['CD']], \n",
    "                'tall': [[], ['CD']], \n",
    "                'rich': [['MONEY'], ['CD']], \n",
    "                'large': [[], ['CD']], \n",
    "                }\n",
    "\n",
    "QUERY_CLASS = {WHAT: [[], ['NN']], \n",
    "               WHO: [['PERSON', 'ORGANIZATION'], ['NN']], \n",
    "               WHERE: [['LOCATION'], ['NN']],\n",
    "               WHEN: [['DATE', 'TIME'], ['CD', 'NN']],\n",
    "               WHICH: [['PERSON', 'LOCATION', 'DATE', 'TIME', 'ORGANIZATION'], ['NN']],\n",
    "               NAME: [['PERSON', 'LOCATION', 'ORGANIZATION'], ['NN']],\n",
    "               #'why': reason, find \"reason word\" \n",
    "               WHOM: [['PERSON', 'ORGANIZATION'], ['NN']], \n",
    "               \n",
    "               # LOCATION, sometimes 'CD' also included as street number, unit number \n",
    "               # what: if and not in query terms, then \n",
    "              \n",
    "              }\n",
    "\n",
    "inverted_index_dict = {} # Store the processed documents' inverted index to improve the effiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ner_dir = '/Users/yifan/Desktop/stanford-ner-2018-02-27/'\n",
    "ner_jarfile = ner_dir + 'stanford-ner.jar'\n",
    "ner_modelfile = ner_dir + 'classifiers/english.all.3class.distsim.crf.ser.gz'\n",
    "ner_tagger = StanfordNERTagger(model_filename=ner_modelfile, path_to_jar=ner_jarfile)\n",
    "\n",
    "pos_dir = '/Users/yifan/Desktop/stanford-postagger-2018-02-27/'\n",
    "pos_modelfile = pos_dir + 'models/english-bidirectional-distsim.tagger'\n",
    "pos_jarfile = pos_dir + 'stanford-postagger.jar'\n",
    "pos_tagger = StanfordPOSTagger(model_filename=pos_modelfile, path_to_jar=pos_jarfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def lemmatize_word(word):\n",
    "    word = word.lower()\n",
    "    lemma_n = lemmatizer.lemmatize(word, 'n')\n",
    "    lemma_v = lemmatizer.lemmatize(word, 'v')\n",
    "    \n",
    "    # If both change, return the shorter one\n",
    "    # If only one change, return the changed one\n",
    "    # If neither change, return the original word\n",
    "    \n",
    "    if lemma_n != word and lemma_v != word:    \n",
    "        if len(lemma_n) < len(lemma_v):\n",
    "            return lemma_n\n",
    "        else:\n",
    "            return lemma_v\n",
    "    elif lemma_n == word and lemma_v == word:\n",
    "        return word\n",
    "    elif lemma_n != word:\n",
    "        return lemma_n\n",
    "    elif lemma_v != word:\n",
    "        return lemma_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Select the most relevant paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def extract_query(query):\n",
    "    tokenized = nltk.word_tokenize(query)\n",
    "    tokenized = [x.lower() for x in tokenized]\n",
    "    q_type = None\n",
    "    \n",
    "    if WHAT in tokenized:\n",
    "        q_type = WHAT\n",
    "                \n",
    "    elif WHO in tokenized:\n",
    "        q_type = WHO\n",
    "        \n",
    "    elif HOW in tokenized: \n",
    "        q_type = HOW\n",
    "        # Since HOW queries have many answer types, furthrer analysis required\n",
    "#         how_index = tokenized.index(HOW)\n",
    "            \n",
    "#         how_sub = tokenized[how_index + 1]\n",
    "        \n",
    "#         if how_sub in HOW_SUBCLASS.keys():\n",
    "#             tokenized.pop(how_index + 1)\n",
    "#             q_type = how_sub\n",
    "#         else:\n",
    "\n",
    "    elif WHERE in tokenized:\n",
    "        q_type = WHERE\n",
    "     \n",
    "    elif WHEN in tokenized:\n",
    "        # Use SUTime\n",
    "        q_type = WHEN\n",
    "    \n",
    "    elif WHICH in tokenized:\n",
    "        q_type = WHICH\n",
    "        \n",
    "    elif WHY in tokenized:\n",
    "        q_type = WHY\n",
    "    \n",
    "    elif WHOM in tokenized:\n",
    "        q_type = WHOM\n",
    "    \n",
    "    elif NAME in tokenized:\n",
    "        q_type = NAME\n",
    "    else:\n",
    "        if 'or' in tokenized:\n",
    "            q_type = BINARY\n",
    "        else:\n",
    "            q_type = POLAR\n",
    "        \n",
    "    terms = []\n",
    "    for token in tokenized:\n",
    "        if token not in stopwords and token not in string.punctuation: \n",
    "            terms.append(lemmatize_word(token))\n",
    "            \n",
    "    return terms, q_type "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def select_document(fname, docid):\n",
    "    with open(fname) as json_data:\n",
    "        infile = json.load(json_data)\n",
    "        for doc in infile:\n",
    "            if doc['docid'] == docid:\n",
    "                text = doc['text']\n",
    "    return text\n",
    "\n",
    "# def extract_query_terms(query, q_type):\n",
    "#     q_terms = extract_terms(query)\n",
    "#     # Since HOW queries have many answer types, furthrer analysis required\n",
    "#     if q_type == 'HOW':\n",
    "        \n",
    "\n",
    "def parse_paragraphs(text):\n",
    "    identifier = 0\n",
    "    for para in text:\n",
    "        yield (identifier, para)\n",
    "        identifier += 1\n",
    "\n",
    "# def extract_terms(para):\n",
    "#     terms = set()\n",
    "#     for token in nltk.word_tokenize(para):\n",
    "#         if token not in stopwords: # 'in' and 'not in' opera]tions are much faster over sets that lists\n",
    "#             terms.add(lemmatize_word(token))\n",
    "#     return terms\n",
    "\n",
    "def extract_term_freqs(para):\n",
    "    tfs = Counter()\n",
    "    for token in nltk.word_tokenize(para):\n",
    "        if token not in stopwords and token not in string.punctuation: # 'in' and 'not in' operations are much faster over sets that lists\n",
    "            tfs[lemmatize_word(token)] += 1\n",
    "    return tfs\n",
    "\n",
    "def compute_para_freqs(para_term_freqs):\n",
    "    pfs = Counter()\n",
    "    for tfs in para_term_freqs.values():\n",
    "        for term in tfs.keys():\n",
    "            pfs[term] += 1\n",
    "    return pfs\n",
    "\n",
    "def get_index(para_term_freqs, para_freqs, M):\n",
    "    vsm_inverted_index = defaultdict(list)\n",
    "    for pid, term_freqs in para_term_freqs.items():\n",
    "        N = sum(term_freqs.values())\n",
    "        length = 0\n",
    "\n",
    "        # find tf*idf values and accumulate sum of squares \n",
    "        tfidf_values = []\n",
    "        for term, count in term_freqs.items():\n",
    "            tfidf = float(count) / N * log(M / float(para_freqs[term]))\n",
    "            tfidf_values.append((term, tfidf))\n",
    "            length += tfidf ** 2\n",
    "\n",
    "        # normalise documents by length and insert into index\n",
    "        length = length ** 0.5\n",
    "        for term, tfidf in tfidf_values:\n",
    "            # note the inversion of the indexing, to be term -> (doc_id, score)\n",
    "            vsm_inverted_index[term].append([pid, tfidf / length])\n",
    "\n",
    "    # ensure posting lists are in sorted order (less important here cf above)\n",
    "    for term, pids in vsm_inverted_index.items():\n",
    "        pids.sort()\n",
    "    return vsm_inverted_index\n",
    "    \n",
    "def query_vsm(query, index, k=4): # Only return the most revelant paragraph\n",
    "    accumulator = Counter()\n",
    "    for term in query:\n",
    "        postings = index[term]\n",
    "        for pid, weight in postings:\n",
    "            accumulator[pid] += weight\n",
    "    return accumulator.most_common(k)\n",
    "\n",
    "def get_freq_dict(raw_paras):\n",
    "#     paras = {}\n",
    "#     for pid, p in raw_paras:\n",
    "#         terms = extract_terms(p)\n",
    "#         paras[pid] = terms\n",
    "\n",
    "    para_term_freqs = {}\n",
    "    for pid, para in raw_paras:\n",
    "        term_freqs = extract_term_freqs(para)\n",
    "        para_term_freqs[pid] = term_freqs\n",
    "    M = len(para_term_freqs)\n",
    "    return para_term_freqs, M\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_para(docid, query):\n",
    "    text = select_document(DOCFNAME, docid)\n",
    "    raw_paras = list(parse_paragraphs(text))\n",
    "    if docid not in inverted_index_dict.keys(): # check whether the document has been processed before\n",
    "#         text = select_document(DOCFNAME, docid)\n",
    "#         raw_paras = list(parse_paragraphs(text))\n",
    "\n",
    "        para_term_freqs, M = get_freq_dict(raw_paras)\n",
    "        para_freqs = compute_para_freqs(para_term_freqs)\n",
    "        inverted_index = get_index(para_term_freqs, para_freqs, M)\n",
    "        inverted_index_dict[docid] = inverted_index\n",
    "    else:\n",
    "        inverted_index = inverted_index_dict[docid]\n",
    "    results = query_vsm(query, inverted_index)\n",
    "    if results == []:\n",
    "        return 0, raw_paras[0][1]\n",
    "    else:\n",
    "        return results[0][0], raw_paras[results[0][0]][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_sent(result_para, query):\n",
    "    sentences = sent_segmenter.tokenize(result_para)\n",
    "    raw_sents = list(parse_paragraphs(sentences))\n",
    "    sent_term_freqs, M = get_freq_dict(raw_sents)\n",
    "    sent_freqs = compute_para_freqs(sent_term_freqs)\n",
    "    results = query_vsm(query, get_index(sent_term_freqs, sent_freqs, M))\n",
    "    if results == []:\n",
    "        return raw_sents[0][1]\n",
    "    else:\n",
    "        return raw_sents[results[0][0]][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_ans(my_sent, query, q_type):\n",
    "    \n",
    "    ans = ''\n",
    "    \n",
    "    if q_type == WHAT:\n",
    "        ans = get_what_ans(my_sent, query)\n",
    "    elif q_type == WHO:\n",
    "        ans = get_who_ans(my_sent, query)\n",
    "    elif q_type == HOW:  \n",
    "        ans = get_how_ans(my_sent, query)\n",
    "    elif q_type == WHERE:\n",
    "        ans = get_where_ans(my_sent, query)\n",
    "    elif q_type == WHEN:\n",
    "        ans = get_when_ans(my_sent, query)\n",
    "    elif q_type == WHICH:\n",
    "        ans = get_which_ans(my_sent, query)\n",
    "    elif q_type == WHY:\n",
    "        ans = get_why_ans(my_sent, query)\n",
    "    elif q_type == WHOM:\n",
    "        ans = get_whom_ans(my_sent, query)\n",
    "    elif q_type == NAME:\n",
    "        ans = get_name_ans(my_sent, query)\n",
    "    elif q_type == BINARY:\n",
    "        ans = get_binary_ans(my_sent, query)\n",
    "    elif q_type == POLAR:\n",
    "        ans = get_polar_ans(my_sent, query)  \n",
    "        \n",
    "    if ans = '':\n",
    "        return my_sent\n",
    "    else:\n",
    "        return ans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_what_ans(my_sent, query):\n",
    "    sent_splt = nltk.word_tokenize(my_sent)\n",
    "    sent_splt_lemma = [lemmatize_word(x) for x in sent_splt]\n",
    "    \n",
    "    pos_candidate = tag_sent(sent_splt, sent_splt_lemma, query, POS)\n",
    "    \n",
    "    pos_tags = QUERY_CLASS[WHAT][POS]\n",
    "    \n",
    "    # special case: \"what color\"\n",
    "#     if sent_splt_lemma[sent_splt_lemma.index(WHAT)+1] == 'color':\n",
    "#         pos_tags.append('JJ')\n",
    "    \n",
    "    ans_candidate = get_ans_candidate(pos_candidate, pos_tags)\n",
    "    \n",
    "    if ans_candidate == None:\n",
    "        return my_sent\n",
    "    else:\n",
    "        return get_ans_string(sent_splt, ans_candidate)\n",
    "    \n",
    "# def get_who_ans(my_sent, query):\n",
    "    \n",
    "    \n",
    "# def get_basic_how_ans(my_sent, query):\n",
    "    \n",
    "    \n",
    "# def get_where_ans(my_sent, query):\n",
    "    \n",
    "    \n",
    "# def get_when_ans(my_sent, query):\n",
    "    \n",
    "    \n",
    "# def get_which_ans(my_sent, query):\n",
    "    \n",
    "    \n",
    "# def get_why_ans(my_sent, query):\n",
    "    \n",
    "    \n",
    "# def get_whom_ans(my_sent, query):\n",
    "    \n",
    "    \n",
    "# def get_name_ans(my_sent, query):\n",
    "    \n",
    "    \n",
    "# def get_binary_ans(my_sent, query):\n",
    "    \n",
    "    \n",
    "# def get_polar_ans(my_sent, query):\n",
    "    \n",
    "    \n",
    "# def get_how_ans(my_sent, query, q_type):\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tag_sent(sent_splt, sent_splt_lemma, query, tag_type):\n",
    "    \n",
    "    if tag_type == NER:\n",
    "        tagger = ner_tagger\n",
    "    else:\n",
    "        tagger = pos_tagger\n",
    "    \n",
    "    tagged = tagger.tag(sent_splt)\n",
    "    \n",
    "    candidates = defaultdict(list)\n",
    "    \n",
    "    for i in range(len(sent_splt_lemma)):\n",
    "        word = sent_splt_lemma[i]\n",
    "        if word not in query and word not in stopwords and word not in string.punctuation:\n",
    "                if tag_type == NER:\n",
    "                    candidates[tagged[i][1]].append(i)\n",
    "                else:\n",
    "                    candidates[tagged[i][1][:2]].append(i)\n",
    "    return candidates\n",
    "\n",
    "def get_ans_candidate(candidate, tags):\n",
    "    ans = []\n",
    "    \n",
    "    for tag in tags:\n",
    "        if candidate.has_key(tag):\n",
    "            ans = candidate[tag]\n",
    "            \n",
    "    if ans == []:\n",
    "        for lst in candidate.values():\n",
    "            ans += lst\n",
    "        \n",
    "    return ans\n",
    "      \n",
    "def get_ans_string(sent_splt, ans_candidate):\n",
    "    words = [sent_splt[i].lower() for i in ans_candidate]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "red\n"
     ]
    }
   ],
   "source": [
    "my_sent = \"The apple is red.\"\n",
    "query = \"What is the color of an apple?\"\n",
    "print get_what_ans(my_sent, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
