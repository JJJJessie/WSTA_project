{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Team Name:     XiaoJiLingGui\n",
    "\n",
    "Kaggle Login:  siqiguo\n",
    "\n",
    "Student Name:  Siqi Guo\n",
    "\n",
    "Student ID:    743053\n",
    "\n",
    "Kaggle Login: \n",
    "\n",
    "Student Name:\n",
    "\n",
    "Student ID: \n",
    "\n",
    "Python version used: 2.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from math import log\n",
    "from collections import defaultdict, Counter\n",
    "import requests, tarfile\n",
    "import json\n",
    "\n",
    "stopwords = set(nltk.corpus.stopwords.words('english')) # wrap in a set() (see below)\n",
    "stemmer = nltk.stem.PorterStemmer() \n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer() \n",
    "sent_segmenter = nltk.data.load('tokenizers/punkt/english.pickle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def lemmatize_word(word):\n",
    "    word = word.lower()\n",
    "    lemma_n = lemmatizer.lemmatize(word,'n')\n",
    "    lemma_v = lemmatizer.lemmatize(word,'v')\n",
    "    if len(lemma_n) <= len(lemma_v):\n",
    "        return lemma_n\n",
    "    else:\n",
    "        return lemma_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'query' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-35e2fea6c4e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcounter\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mdevelfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'project_files/devel.json'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpara_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevelfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-35e2fea6c4e9>\u001b[0m in \u001b[0;36mpara_accuracy\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mdocid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'docid'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mpara\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                 \u001b[0mcounter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'query' is not defined"
     ]
    }
   ],
   "source": [
    "def para_accuracy(fname):\n",
    "    counter = float(0)\n",
    "    with open(fname) as json_data:\n",
    "        infile = json.load(json_data)\n",
    "        length = len(infile)\n",
    "        for dic in infile:\n",
    "            question = dic['question']\n",
    "            ans = dic['text']\n",
    "            para = dic['answer_paragraph']\n",
    "            docid = dic['docid']\n",
    "            \n",
    "            result = get_result(docid, query)\n",
    "            if result == para:\n",
    "                counter += 1\n",
    "    return counter/length\n",
    "develfname = 'project_files/devel.json'\n",
    "acc = para_accuracy(develfname)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Select the most relevant paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def select_document(fname, docid):\n",
    "    with open(fname) as json_data:\n",
    "        infile = json.load(json_data)\n",
    "        for doc in infile:\n",
    "            if doc['docid'] == docid:\n",
    "                text = doc['text']\n",
    "    return text\n",
    "\n",
    "# def extract_query_terms(query, q_type):\n",
    "#     q_terms = extract_terms(query)\n",
    "#     # Since HOW queries have many answer types, furthrer analysis required\n",
    "#     if q_type == 'HOW':\n",
    "        \n",
    "\n",
    "def parse_paragraphs(text):\n",
    "    identifier = 0\n",
    "    for para in text:\n",
    "        yield (identifier, para)\n",
    "        identifier += 1\n",
    "\n",
    "# def extract_terms(para):\n",
    "#     terms = set()\n",
    "#     for token in nltk.word_tokenize(para):\n",
    "#         if token not in stopwords: # 'in' and 'not in' opera]tions are much faster over sets that lists\n",
    "#             terms.add(lemmatize_word(token))\n",
    "#     return terms\n",
    "\n",
    "def extract_term_freqs(para):\n",
    "    tfs = Counter()\n",
    "    for token in nltk.word_tokenize(para):\n",
    "        if token not in stopwords: # 'in' and 'not in' operations are much faster over sets that lists\n",
    "            tfs[lemmatize_word(token)] += 1\n",
    "    return tfs\n",
    "\n",
    "def compute_para_freqs(para_term_freqs):\n",
    "    pfs = Counter()\n",
    "    for tfs in para_term_freqs.values():\n",
    "        for term in tfs.keys():\n",
    "            pfs[term] += 1\n",
    "    return pfs\n",
    "\n",
    "def get_index(para_term_freqs, para_freqs, M):\n",
    "    vsm_inverted_index = defaultdict(list)\n",
    "    for pid, term_freqs in para_term_freqs.items():\n",
    "        N = sum(term_freqs.values())\n",
    "        length = 0\n",
    "\n",
    "        # find tf*idf values and accumulate sum of squares \n",
    "        tfidf_values = []\n",
    "        for term, count in term_freqs.items():\n",
    "            tfidf = float(count) / N * log(M / float(para_freqs[term]))\n",
    "            tfidf_values.append((term, tfidf))\n",
    "            length += tfidf ** 2\n",
    "\n",
    "        # normalise documents by length and insert into index\n",
    "        length = length ** 0.5\n",
    "        for term, tfidf in tfidf_values:\n",
    "            # note the inversion of the indexing, to be term -> (doc_id, score)\n",
    "            vsm_inverted_index[term].append([pid, tfidf / length])\n",
    "\n",
    "    # ensure posting lists are in sorted order (less important here cf above)\n",
    "    for term, pids in vsm_inverted_index.items():\n",
    "        pids.sort()\n",
    "    return vsm_inverted_index\n",
    "    \n",
    "def query_vsm(query, index, k=2):\n",
    "    accumulator = Counter()\n",
    "    for term in query:\n",
    "        postings = index[term]\n",
    "        for pid, weight in postings:\n",
    "            accumulator[pid] += weight\n",
    "    return accumulator.most_common(k)\n",
    "\n",
    "def get_freq_dict(raw_paras):\n",
    "#     paras = {}\n",
    "#     for pid, p in raw_paras:\n",
    "#         terms = extract_terms(p)\n",
    "#         paras[pid] = terms\n",
    "\n",
    "    para_term_freqs = {}\n",
    "    for pid, para in raw_paras:\n",
    "        term_freqs = extract_term_freqs(para)\n",
    "        para_term_freqs[pid] = term_freqs\n",
    "    M = len(para_term_freqs)\n",
    "    return para_term_freqs, M\n",
    "\n",
    "\n",
    "# def get_result(docid, query):\n",
    "#     docfname = 'project_files/documents.json'\n",
    "#     text = select_document(docfname, docid)\n",
    "#     raw_paras = list(parse_paragraphs(text))\n",
    "\n",
    "#     paras, para_term_freqs, M = get_freq_dict(raw_paras)\n",
    "#     para_freqs = compute_para_freqs(para_term_freqs)\n",
    "#     results = query_vsm([stemmer.stem(term.lower()) for term in \"constant element particle\".split()], \n",
    "#                         get_index(para_term_freqs, para_freqs, M))\n",
    "#     return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_result(docid, query):\n",
    "    docfname = 'project_files/documents.json'\n",
    "    text = select_document(docfname, docid)\n",
    "    raw_paras = list(parse_paragraphs(text))\n",
    "\n",
    "    para_term_freqs, M = get_freq_dict(raw_paras)\n",
    "    para_freqs = compute_para_freqs(para_term_freqs)\n",
    "    results = query_vsm([stemmer.stem(term.lower()) for term in \"constant element particle\".split()], \n",
    "                        get_index(para_term_freqs, para_freqs, M))\n",
    "    return results, raw_paras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.1541752855626096), (20, 0.08236155155447321)]\n",
      "(0, u'First recognized in 1900 by Max Planck, it was originally the proportionality constant between the minimal increment of energy, E, of a hypothetical electrically charged oscillator in a cavity that contained black body radiation, and the frequency, f, of its associated electromagnetic wave. In 1905 the value E, the minimal energy increment of a hypothetical oscillator, was theoretically associated by Einstein with a \"quantum\" or minimal element of the energy of the electromagnetic wave itself. The light quantum behaved in some respects as an electrically neutral particle, as opposed to an electromagnetic wave. It was eventually called the photon.')\n"
     ]
    }
   ],
   "source": [
    "result, raw_paras = get_result(0, 'particle, element, constant')\n",
    "# print raw_paras\n",
    "print result\n",
    "print(raw_paras[result[0][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
