{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Team Name:     XiaoJiLingGui\n",
    "\n",
    "Kaggle Login:  siqiguo\n",
    "\n",
    "Student Name:  Siqi Guo\n",
    "\n",
    "Student ID:    743053\n",
    "\n",
    "Kaggle Login: yifan66\n",
    "\n",
    "Student Name: Yifan Wang\n",
    "\n",
    "Student ID: 784386\n",
    "\n",
    "Python version used: 2.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import json, string\n",
    "from math import log\n",
    "from collections import defaultdict, Counter\n",
    "import csv\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tag.stanford import StanfordPOSTagger\n",
    "\n",
    "stopwords = set(nltk.corpus.stopwords.words('english')) # wrap in a set() (see below)\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer() \n",
    "sent_segmenter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "DOCFNAME = 'project_files/documents.json'\n",
    "\n",
    "WHAT = 'what'\n",
    "WHO = 'who'\n",
    "HOW = 'how'\n",
    "WHERE = 'where'\n",
    "WHEN = 'when'\n",
    "WHICH = 'which'\n",
    "NAME = 'name'\n",
    "WHY = 'why'\n",
    "WHOM = 'whom'\n",
    "BINARY = 'binary'\n",
    "POLAR = 'polar'\n",
    "\n",
    "PUNCTUATION = string.punctuation.translate(None, '$%')\n",
    "\n",
    "WHAT_CD_WORDS = ['year', 'date', 'percentage', 'value', 'margin']\n",
    "\n",
    "NER = 0\n",
    "\n",
    "POS = 1\n",
    "\n",
    "NOT_WORDS = ['not', \"n't\"]\n",
    "\n",
    "REASON_WORDS = ['becauce', 'since', 'due to', 'thanks to', 'owing to', \n",
    "                'on account of', 'result', 'for']\n",
    "\n",
    "HOW_SUBCLASS = {'many': [[], ['CD', 'FW']], \n",
    "                'long': [['DURATION'], ['CD', 'FW']], \n",
    "                'much': [['MONEY'], ['$', 'CD', 'FW', 'NN']], \n",
    "                'far': [[], ['CD', 'FW']], \n",
    "                'tall': [[], ['CD', 'FW']], \n",
    "                'rich': [['MONEY'], ['$', 'CD', 'FW']], \n",
    "                'large': [[], ['CD', 'FW']], \n",
    "                }\n",
    "\n",
    "QUERY_CLASS = {WHAT: [[], ['NN', 'FW']],\n",
    "               # To optimise, not include WHO - 'ORGANIZATION' in the tag\n",
    "               WHO: [['PERSON'], ['NN', 'FW']], \n",
    "               WHERE: [['LOCATION'], ['NN', 'FW']],\n",
    "               WHEN: [['DATE', 'TIME'], ['CD', 'NN', 'FW']],\n",
    "               WHICH: [['PERSON', 'LOCATION', 'DATE', 'TIME', 'ORGANIZATION'], ['NN', 'FW']],\n",
    "               NAME: [['PERSON', 'LOCATION', 'ORGANIZATION'], ['NN', 'FW']],\n",
    "               #'why': reason, find \"reason word\" \n",
    "               WHOM: [['PERSON'], ['NN', 'FW']],\n",
    "               \n",
    "               # LOCATION, sometimes 'CD' also included as street number, unit number \n",
    "               # what: if and not in query terms, then \n",
    "              \n",
    "              }\n",
    "\n",
    "inverted_index_dict = {} # Store the processed documents' inverted index to improve the effiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ner_dir = 'stanford-ner-2018-02-27/'\n",
    "ner_jarfile = ner_dir + 'stanford-ner.jar'\n",
    "ner_modelfile = ner_dir + 'classifiers/english.all.3class.distsim.crf.ser.gz'\n",
    "ner_tagger = StanfordNERTagger(model_filename=ner_modelfile, path_to_jar=ner_jarfile)\n",
    "\n",
    "pos_dir = 'stanford-postagger-2018-02-27/'\n",
    "pos_modelfile = pos_dir + 'models/english-bidirectional-distsim.tagger'\n",
    "pos_jarfile = pos_dir + 'stanford-postagger.jar'\n",
    "pos_tagger = StanfordPOSTagger(model_filename=pos_modelfile, path_to_jar=pos_jarfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def lemmatize_word(word):\n",
    "    word = word.lower()\n",
    "#     print word\n",
    "    adj_word = get_adj(word)\n",
    "    if adj_word != None:\n",
    "        return adj_word\n",
    "    lemma_n = lemmatizer.lemmatize(word, 'n')\n",
    "    lemma_v = lemmatizer.lemmatize(word, 'v')\n",
    "    \n",
    "    # If both change, return the shorter one\n",
    "    # If only one change, return the changed one\n",
    "    # If neither change, return the original word\n",
    "    \n",
    "    if lemma_n != word and lemma_v != word:    \n",
    "        if len(lemma_n) < len(lemma_v):\n",
    "            return lemma_n\n",
    "        else:\n",
    "            return lemma_v\n",
    "    elif lemma_n == word and lemma_v == word:\n",
    "        return word\n",
    "    elif lemma_n != word:\n",
    "        return lemma_n\n",
    "    elif lemma_v != word:\n",
    "        return lemma_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_adj(word):\n",
    "    for syn in wn.synsets(word):\n",
    "        syn_split = syn.name().split('.')\n",
    "        if syn_split[0] == word and syn_split[1] == 'r':\n",
    "            for lemmas in syn.lemmas(): # all possible lemmas\n",
    "                if lemmas.name() == word:\n",
    "                    if lemmas.pertainyms() == []:\n",
    "                        return None\n",
    "                    return lemmas.pertainyms()[0].name()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# lemmatize_word('lakes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Select the most relevant paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def extract_query(query):\n",
    "    tokenized = nltk.word_tokenize(query)\n",
    "    tokenized = [x.lower() for x in tokenized]\n",
    "    q_type = None\n",
    "    \n",
    "    if WHAT in tokenized:\n",
    "        q_type = WHAT\n",
    "                \n",
    "    elif WHO in tokenized:\n",
    "        q_type = WHO\n",
    "        \n",
    "    elif HOW in tokenized: \n",
    "        q_type = HOW\n",
    "        # Since HOW queries have many answer types, furthrer analysis required\n",
    "#         how_index = tokenized.index(HOW)\n",
    "            \n",
    "#         how_sub = tokenized[how_index + 1]\n",
    "        \n",
    "#         if how_sub in HOW_SUBCLASS.keys():\n",
    "#             tokenized.pop(how_index + 1)\n",
    "#             q_type = how_sub\n",
    "#         else:\n",
    "\n",
    "    elif WHERE in tokenized:\n",
    "        q_type = WHERE\n",
    "     \n",
    "    elif WHEN in tokenized:\n",
    "        # Use SUTime\n",
    "        q_type = WHEN\n",
    "    \n",
    "    elif WHICH in tokenized:\n",
    "        q_type = WHICH\n",
    "        \n",
    "    elif WHY in tokenized:\n",
    "        q_type = WHY\n",
    "    \n",
    "    elif WHOM in tokenized:\n",
    "        q_type = WHOM\n",
    "    \n",
    "    elif NAME in tokenized:\n",
    "        q_type = NAME\n",
    "    else:\n",
    "        if 'or' in tokenized:\n",
    "            q_type = BINARY\n",
    "        else:\n",
    "            q_type = POLAR\n",
    "        \n",
    "    terms = []\n",
    "    for token in tokenized:\n",
    "        if token not in stopwords and token not in PUNCTUATION: \n",
    "            terms.append(lemmatize_word(token))\n",
    "            \n",
    "    return terms, q_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def select_document(fname, docid):\n",
    "    with open(fname) as json_data:\n",
    "        infile = json.load(json_data)\n",
    "        for doc in infile:\n",
    "            if doc['docid'] == docid:\n",
    "                text = doc['text']\n",
    "    return text\n",
    "        \n",
    "\n",
    "def parse_paragraphs(text):\n",
    "    identifier = 0\n",
    "    for para in text:\n",
    "        yield (identifier, para)\n",
    "        identifier += 1\n",
    "\n",
    "def extract_term_freqs(para):\n",
    "    tfs = Counter()\n",
    "    for token in nltk.word_tokenize(para):\n",
    "        if token not in stopwords and token not in PUNCTUATION: # 'in' and 'not in' operations are much faster over sets that lists\n",
    "            tfs[lemmatize_word(token)] += 1\n",
    "    return tfs\n",
    "\n",
    "def compute_para_freqs(para_term_freqs):\n",
    "    pfs = Counter()\n",
    "    for tfs in para_term_freqs.values():\n",
    "        for term in tfs.keys():\n",
    "            pfs[term] += 1\n",
    "    return pfs\n",
    "\n",
    "def get_index(para_term_freqs, para_freqs, M):\n",
    "    vsm_inverted_index = defaultdict(list)\n",
    "    for pid, term_freqs in para_term_freqs.items():\n",
    "        N = sum(term_freqs.values())\n",
    "        length = 0\n",
    "\n",
    "        # find tf*idf values and accumulate sum of squares \n",
    "        tfidf_values = []\n",
    "        for term, count in term_freqs.items():\n",
    "            tfidf = float(count) / N * log(M / float(para_freqs[term]))\n",
    "            tfidf_values.append((term, tfidf))\n",
    "            length += tfidf ** 2\n",
    "\n",
    "        # normalise documents by length and insert into index\n",
    "        length = length ** 0.5\n",
    "        for term, tfidf in tfidf_values:\n",
    "            # note the inversion of the indexing, to be term -> (doc_id, score)\n",
    "            vsm_inverted_index[term].append([pid, tfidf / length])\n",
    "\n",
    "    # ensure posting lists are in sorted order (less important here cf above)\n",
    "    for term, pids in vsm_inverted_index.items():\n",
    "        pids.sort()\n",
    "    return vsm_inverted_index\n",
    "    \n",
    "def query_vsm(query, index, k=4): # Only return the most revelant paragraph\n",
    "    accumulator = Counter()\n",
    "    for term in query:\n",
    "        postings = index[term]\n",
    "        for pid, weight in postings:\n",
    "            accumulator[pid] += weight\n",
    "    return accumulator.most_common(k)\n",
    "\n",
    "def get_freq_dict(raw_paras):\n",
    "    para_term_freqs = {}\n",
    "    for pid, para in raw_paras:\n",
    "        term_freqs = extract_term_freqs(para)\n",
    "        para_term_freqs[pid] = term_freqs\n",
    "    M = len(para_term_freqs)\n",
    "    return para_term_freqs, M\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_para(docid, query):\n",
    "    text = select_document(DOCFNAME, docid)\n",
    "    raw_paras = list(parse_paragraphs(text))\n",
    "    if len(raw_paras) == 1: # If there's only one paragraph in the document\n",
    "        return 0, raw_paras[0][1]\n",
    "    if docid not in inverted_index_dict.keys(): # check whether the document has been processed before\n",
    "        para_term_freqs, M = get_freq_dict(raw_paras)\n",
    "        \n",
    "        para_freqs = compute_para_freqs(para_term_freqs)\n",
    "        inverted_index = get_index(para_term_freqs, para_freqs, M)\n",
    "        inverted_index_dict[docid] = inverted_index\n",
    "    else:\n",
    "        inverted_index = inverted_index_dict[docid]\n",
    "    results = query_vsm(query, inverted_index)\n",
    "    if results == []:\n",
    "        return 0, raw_paras[0][1]\n",
    "    else:\n",
    "        return results[0][0], raw_paras[results[0][0]][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_sent(result_para, query):\n",
    "    sentences = sent_segmenter.tokenize(result_para)\n",
    "    raw_sents = list(parse_paragraphs(sentences))\n",
    "    if len(raw_sents) == 1: # If there's only one sentence in the paragraph\n",
    "        return raw_sents[0][1]\n",
    "    sent_term_freqs, M = get_freq_dict(raw_sents)\n",
    "    sent_freqs = compute_para_freqs(sent_term_freqs)\n",
    "    results = query_vsm(query, get_index(sent_term_freqs, sent_freqs, M))\n",
    "    if results == []:\n",
    "        return raw_sents[0][1]\n",
    "    else:\n",
    "        return raw_sents[results[0][0]][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_ans(my_sent, query, q_type):\n",
    "    \n",
    "    ans = ''\n",
    "    \n",
    "    if q_type == WHAT:\n",
    "        ans = get_what_ans(my_sent, query)\n",
    "    elif q_type == WHO:\n",
    "        ans = get_who_ans(my_sent, query)\n",
    "    elif q_type == HOW:  \n",
    "        ans = get_how_ans(my_sent, query)\n",
    "    elif q_type == WHERE:\n",
    "        ans = get_where_ans(my_sent, query)\n",
    "    elif q_type == WHEN:\n",
    "        ans = get_when_ans(my_sent, query)\n",
    "    elif q_type == WHICH:\n",
    "        ans = get_which_ans(my_sent, query)\n",
    "    elif q_type == WHY:\n",
    "        ans = get_why_ans(my_sent, query)\n",
    "    elif q_type == WHOM:\n",
    "        ans = get_whom_ans(my_sent, query)\n",
    "    elif q_type == NAME:\n",
    "        ans = get_name_ans(my_sent, query)\n",
    "    elif q_type == BINARY:\n",
    "        ans = get_binary_ans(my_sent, query)\n",
    "    elif q_type == POLAR:\n",
    "        ans = get_polar_ans(my_sent, query)  \n",
    "        \n",
    "    if ans == '':\n",
    "        return my_sent\n",
    "    else:\n",
    "        return ans\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_what_ans(my_sent, query):\n",
    "    sent_splt = nltk.word_tokenize(my_sent)\n",
    "    sent_splt_lemma = [lemmatize_word(x) for x in sent_splt]\n",
    "    \n",
    "    pos_candidate = tag_sent(sent_splt, sent_splt_lemma, query, POS)\n",
    "    \n",
    "    pos_tags = QUERY_CLASS[WHAT][POS][:]\n",
    "#     print pos_tags\n",
    "    \n",
    "    # special case: \"what color\"\n",
    "    if 'color' in query:\n",
    "        pos_tags.append('JJ')\n",
    "\n",
    "    # special case: what for an answer include numbers\n",
    "    for word in query:\n",
    "#         print pos_tags\n",
    "        if word in WHAT_CD_WORDS:\n",
    "            pos_tags.append('CD')\n",
    "            pos_tags.remove('NN')\n",
    "            break\n",
    "    \n",
    "    ans_candidate = get_ans_candidate(pos_candidate, pos_tags, POS)\n",
    "\n",
    "    if 'percentage' in query and '%' in sent_splt_lemma:\n",
    "        ans_candidate.append(sent_splt_lemma.index('%'))\n",
    "    \n",
    "    return get_ans_string(sent_splt, ans_candidate)\n",
    "    \n",
    "def get_who_ans(my_sent, query):\n",
    "    sent_splt = nltk.word_tokenize(my_sent)\n",
    "    sent_splt_lemma = [lemmatize_word(x) for x in sent_splt]\n",
    "    \n",
    "    ner_candidate = tag_sent(sent_splt, sent_splt_lemma, query, NER)\n",
    "    \n",
    "    ner_tags = QUERY_CLASS[WHO][NER][:]\n",
    "    \n",
    "    ans_candidate = get_ans_candidate(ner_candidate, ner_tags, NER)\n",
    "    \n",
    "    if ans_candidate == []:\n",
    "        pos_candidate = tag_sent(sent_splt, sent_splt_lemma, query, POS)\n",
    "        pos_tags = QUERY_CLASS[WHO][POS][:]\n",
    "        ans_candidate = get_ans_candidate(pos_candidate, pos_tags, POS)\n",
    "        \n",
    "    return get_ans_string(sent_splt, ans_candidate)\n",
    "    \n",
    "def get_where_ans(my_sent, query):\n",
    "    sent_splt = nltk.word_tokenize(my_sent)\n",
    "    sent_splt_lemma = [lemmatize_word(x) for x in sent_splt]\n",
    "    \n",
    "    ner_candidate = tag_sent(sent_splt, sent_splt_lemma, query, NER)\n",
    "    \n",
    "    ner_tags = QUERY_CLASS[WHERE][NER][:]\n",
    "    \n",
    "    ans_candidate = get_ans_candidate(ner_candidate, ner_tags, NER)\n",
    "    \n",
    "    if ans_candidate == []:\n",
    "        pos_candidate = tag_sent(sent_splt, sent_splt_lemma, query, POS)\n",
    "        pos_tags = QUERY_CLASS[WHERE][POS][:]\n",
    "        ans_candidate = get_ans_candidate(pos_candidate, pos_tags, POS)\n",
    "        \n",
    "    return get_ans_string(sent_splt, ans_candidate)\n",
    "    \n",
    "def get_when_ans(my_sent, query):\n",
    "    sent_splt = nltk.word_tokenize(my_sent)\n",
    "    sent_splt_lemma = [lemmatize_word(x) for x in sent_splt]\n",
    "    \n",
    "    ner_candidate = tag_sent(sent_splt, sent_splt_lemma, query, NER)\n",
    "    \n",
    "    ner_tags = QUERY_CLASS[WHEN][NER][:]\n",
    "    \n",
    "    ans_candidate = get_ans_candidate(ner_candidate, ner_tags, NER)\n",
    "    \n",
    "    if ans_candidate == []:\n",
    "        pos_candidate = tag_sent(sent_splt, sent_splt_lemma, query, POS)\n",
    "        pos_tags = QUERY_CLASS[WHEN][POS][:]\n",
    "        ans_candidate = get_ans_candidate(pos_candidate, pos_tags, POS)\n",
    "        \n",
    "    return get_ans_string(sent_splt, ans_candidate)\n",
    "    \n",
    "def get_which_ans(my_sent, query):\n",
    "    sent_splt = nltk.word_tokenize(my_sent)\n",
    "    sent_splt_lemma = [lemmatize_word(x) for x in sent_splt]\n",
    "    \n",
    "    ner_candidate = tag_sent(sent_splt, sent_splt_lemma, query, NER)\n",
    "    \n",
    "    ner_tags = QUERY_CLASS[WHICH][NER][:]\n",
    "    \n",
    "    ans_candidate = get_ans_candidate(ner_candidate, ner_tags, NER)\n",
    "    \n",
    "    if ans_candidate == []:\n",
    "        pos_candidate = tag_sent(sent_splt, sent_splt_lemma, query, POS)\n",
    "        pos_tags = QUERY_CLASS[WHICH][POS][:]\n",
    "        ans_candidate = get_ans_candidate(pos_candidate, pos_tags, POS)\n",
    "        \n",
    "    return get_ans_string(sent_splt, ans_candidate)\n",
    "\n",
    "    \n",
    "def get_why_ans(my_sent, query):\n",
    "    \n",
    "    for word in REASON_WORDS:\n",
    "        if word in my_sent:\n",
    "            my_sent = my_sent.split(word,1)[1] \n",
    "            \n",
    "    sent_splt = nltk.word_tokenize(my_sent)\n",
    "    \n",
    "    # special case - \"on one's account\" / \"on this account\"\n",
    "    if ('account' in sent_splt and 'on' in sent_splt \n",
    "        and (sent_splt.index('account') == sent_splt.index('on')+2 or \n",
    "             sent_splt.index('account') == sent_splt.index('on')+3)):\n",
    "        \n",
    "        sent_splt.remove('account')\n",
    "        sent_splt.remove('on')\n",
    "        \n",
    "    \n",
    "    sent_splt_lemma = [lemmatize_word(x) for x in sent_splt]\n",
    "    \n",
    "    ans_candidate = []\n",
    "    for i in range(len(sent_splt_lemma)):\n",
    "        word = sent_splt_lemma[i]\n",
    "        if (word not in query and word not in stopwords and \n",
    "            word not in PUNCTUATION):\n",
    "            ans_candidate.append(i)\n",
    "\n",
    "    return get_ans_string(sent_splt, ans_candidate)\n",
    "    \n",
    "def get_whom_ans(my_sent, query):\n",
    "    sent_splt = nltk.word_tokenize(my_sent)\n",
    "    sent_splt_lemma = [lemmatize_word(x) for x in sent_splt]\n",
    "    pos_candidate = tag_sent(sent_splt, sent_splt_lemma, query, POS)\n",
    "\n",
    "    if pos_candidate.has_key('IN'):\n",
    "        prep_index = pos_candidate['IN'].sort()[0]\n",
    "        sent_splt = sent_splt[prep_index+1:]\n",
    "        sent_splt_lemma = sent_splt_lemma[prep_index+1:]\n",
    "        pos_candidate = tag_sent(sent_splt, sent_splt_lemma, query, POS)\n",
    "\n",
    "    ner_candidate = tag_sent(sent_splt, sent_splt_lemma, query, NER)\n",
    "    \n",
    "    ner_tags = QUERY_CLASS[WHOM][NER][:]\n",
    "    \n",
    "    ans_candidate = get_ans_candidate(ner_candidate, ner_tags, NER)\n",
    "    \n",
    "    if ans_candidate == []:\n",
    "        pos_tags = QUERY_CLASS[WHOM][POS][:]\n",
    "        ans_candidate = get_ans_candidate(pos_candidate, pos_tags, POS)\n",
    "        \n",
    "    return get_ans_string(sent_splt, ans_candidate)\n",
    "    \n",
    "def get_name_ans(my_sent, query):\n",
    "    sent_splt = nltk.word_tokenize(my_sent)\n",
    "    sent_splt_lemma = [lemmatize_word(x) for x in sent_splt]\n",
    "    \n",
    "    pos_candidate = tag_sent(sent_splt, sent_splt_lemma, query, POS)\n",
    "    \n",
    "    pos_tags = QUERY_CLASS[NAME][POS][:]\n",
    "    \n",
    "    ans_candidate = get_ans_candidate(pos_candidate, pos_tags, POS)\n",
    "\n",
    "    return get_ans_string(sent_splt, ans_candidate)\n",
    "    \n",
    "    \n",
    "def get_binary_ans(my_sent, query):\n",
    "    sent_splt = nltk.word_tokenize(my_sent)\n",
    "    sent_splt_lemma = [lemmatize_word(x) for x in sent_splt]\n",
    "    print sent_splt\n",
    "    if 'true' in query:\n",
    "        for word in NOT_WORDS:\n",
    "            if word in sent_splt:\n",
    "                return 'false'\n",
    "        return 'true'\n",
    "    \n",
    "    pos_candidate = tag_sent(sent_splt, sent_splt_lemma, query, POS)\n",
    "    \n",
    "    ans_candidate = get_ans_candidate(pos_candidate, [], POS)\n",
    "\n",
    "    return get_ans_string(sent_splt, ans_candidate)\n",
    "\n",
    "\n",
    "def get_polar_ans(my_sent, query):\n",
    "    sent_splt = nltk.word_tokenize(my_sent)\n",
    "    sent_splt_lemma = [lemmatize_word(x) for x in sent_splt]\n",
    "    \n",
    "    for word in NOT_WORDS:\n",
    "        if word in sent_splt:\n",
    "            return 'no'\n",
    "        return 'yes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_how_ans(my_sent, query):\n",
    "    sent_splt = nltk.word_tokenize(my_sent)\n",
    "    sent_splt_lemma = [lemmatize_word(x) for x in sent_splt]\n",
    "        \n",
    "#     print sent_splt, sent_splt_lemma\n",
    "    \n",
    "    word_after_how = None\n",
    "    \n",
    "    for word in HOW_SUBCLASS.keys():\n",
    "        if word in query:\n",
    "            word_after_how = word\n",
    "            break\n",
    "    \n",
    "    if word_after_how:\n",
    "                \n",
    "        ner_candidate = tag_sent(sent_splt, sent_splt_lemma, query, NER)\n",
    "        ner_tags = HOW_SUBCLASS[word_after_how][NER]\n",
    "        ans_candidate = get_ans_candidate(ner_candidate, ner_tags, NER)\n",
    "        \n",
    "        \n",
    "        if ans_candidate == []:\n",
    "\n",
    "            pos_candidate = tag_sent(sent_splt, sent_splt_lemma, query, POS)\n",
    "#             print(\"POS_candidate\")\n",
    "#             print pos_candidate\n",
    "            \n",
    "            pos_tags = HOW_SUBCLASS[word_after_how][POS]\n",
    "#             print(\"POS TAGS\")\n",
    "#             print pos_tags\n",
    "            \n",
    "            ans_candidate = get_ans_candidate(pos_candidate, pos_tags, POS) \n",
    "            \n",
    "    else:\n",
    "        pos_candidate = tag_sent(sent_splt, sent_splt_lemma, query, POS)\n",
    "\n",
    "        if pos_candidate.has_key('IN'):\n",
    "            prep_index = pos_candidate['IN'].sort()[0]\n",
    "            sent_splt = sent_splt[prep_index+1:]\n",
    "            sent_splt_lemma = sent_splt_lemma[prep_index+1:]\n",
    "            pos_candidate = tag_sent(sent_splt, sent_splt_lemma, query, POS)\n",
    "        \n",
    "        ans_candidate = get_ans_candidate(pos_candidate, [], POS)\n",
    "    \n",
    "#     print ans_candidate\n",
    "    return get_ans_string(sent_splt, ans_candidate)\n",
    "\n",
    "def get_ans_candidate(candidate, tags, flag):\n",
    "    ans = []\n",
    "    \n",
    "    for tag in tags:\n",
    "#         print tag\n",
    "        if candidate.has_key(tag):\n",
    "#             print 'yes'\n",
    "            ans += candidate[tag]\n",
    "            # flag is NER (=0), break, get the words with one tag\n",
    "            # flag is POS (=1), not break, get words with all tags\n",
    "            if not flag:\n",
    "                break\n",
    "                \n",
    "    # flag is NER (=0), if no ans, return []\n",
    "    # flag is POS (=1), if no ans, return all words regradless of tags\n",
    "    if flag and ans == []:\n",
    "        for lst in candidate.values():\n",
    "            ans += lst\n",
    "        \n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def output_result(final_ans):\n",
    "    with open('names.csv', 'w') as csvfile:\n",
    "        fieldnames = ['id', 'answer']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for each_dict in final_ans:\n",
    "            writer.writerow({'id': each_dict['id'], 'answer': each_dict['answer']})\n",
    "    csvfile.close()\n",
    "\n",
    "def output_result_train(final_ans):\n",
    "    with open('names.csv', 'w') as csvfile:\n",
    "        fieldnames = ['f_score', 'ans', 'my_ans', 'prec', 'recall', 'Q', 'para', 'docid']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for each_dict in final_ans:\n",
    "            writer.writerow({'f_score': each_dict['f_score'],\n",
    "                             'ans': each_dict['ans'], \n",
    "                             'my_ans': each_dict['my_ans'], \n",
    "                             'prec': each_dict['prec'], \n",
    "                             'recall': each_dict['recall'], \n",
    "                             'Q': each_dict['Q'], \n",
    "                             'para': each_dict['para'], \n",
    "                             'docid': each_dict['docid']})\n",
    "    csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def tag_sent(sent_splt, sent_splt_lemma, query, tag_type):\n",
    "    \n",
    "    if tag_type == NER:\n",
    "        tagger = ner_tagger\n",
    "    else:\n",
    "        tagger = pos_tagger\n",
    "    \n",
    "    tagged = tagger.tag(sent_splt)\n",
    "    \n",
    "#     print tagged\n",
    "    \n",
    "    candidates = defaultdict(list)\n",
    "    \n",
    "    for i in range(len(sent_splt_lemma)):\n",
    "        word = sent_splt_lemma[i]\n",
    "#         print word\n",
    "        if word not in query and word not in stopwords and word not in PUNCTUATION:\n",
    "#                 print(\"yes\")\n",
    "                if tag_type == NER:\n",
    "                    candidates[tagged[i][1]].append(i)\n",
    "                else:\n",
    "                    candidates[tagged[i][1][:2]].append(i)\n",
    "    return candidates\n",
    "   \n",
    "def get_ans_string(sent_splt, ans_candidate):\n",
    "    words = set([sent_splt[i].lower() for i in ans_candidate])\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Below: get accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def get_para_accuracy(fname):\n",
    "#     counter = float(0)\n",
    "#     with open(fname) as json_data:\n",
    "#         infile = json.load(json_data)\n",
    "#         length = len(infile)\n",
    "#         for dic in infile:\n",
    "#             question = dic['question']\n",
    "#             ans = dic['text']\n",
    "#             para = dic['answer_paragraph']\n",
    "#             docid = dic['docid']    \n",
    "            \n",
    "#             query, q_type = extract_query(question)\n",
    "            \n",
    "# #             print question[:20], query\n",
    "            \n",
    "#             para_ord, result = get_para(docid, query)\n",
    "            \n",
    "#             if para_ord == para:\n",
    "#                 counter += 1\n",
    "\n",
    "#     return counter/length\n",
    "\n",
    "# develfname = 'project_files/mytest.json' # Need to change later\n",
    "# acc = get_para_accuracy(develfname)\n",
    "# print acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F_SCORE: 0.740740740741 [ANS] june 16 , 1911 [MY_ANS] 1911 four 16 [Q] On what date did the companies that became the Computing-Tabulating-Recording Company get consolidated?\n",
      "F_SCORE: 1.0 [ANS] 5 % [MY_ANS] % 5 [Q] What percentage of its desktop PCs does IBM plan to install Open Client on to?\n",
      "F_SCORE: 0.727272727273 [ANS] 1946 [MY_ANS] 1964 18 1946 [Q] What year did IBM hire its first black salesman?\n",
      "F_SCORE: 0.122448979592 [ANS] spss [MY_ANS] use agreement weather licensing data channel [Q] IBM made an acquisition in 2009, name it.\n",
      "F_SCORE: 0.769230769231 [ANS] universal product code [MY_ANS] laboratories research [Q] This IBM invention is known by the acronym UPC, what is the full name?\n",
      "F_SCORE: 0.545454545455 [ANS] second largest [MY_ANS] terms ibm capitalization revenue u.s. nineteenth market [Q] In 2012 Fortune ranked the largest US firms by number employees, what was IBMs rank?\n",
      "F_SCORE: 0.666666666667 [ANS] 4,100 gallons [MY_ANS] six 4,100 two one [Q] How many gallons of liquid cleaning agent leaked from an IBM facility in 1979?\n",
      "F_SCORE: 0.352941176471 [ANS] 1924 [MY_ANS] 1963 eight 34 [Q] In what year did IBM get its name?\n",
      "F_SCORE: 0.0980392156863 [ANS] linux [MY_ANS] development web management ibm xml websphere lotus ajax subjects products rational linux services information php tivoli [Q] DeveloperWorks has content about open industry standard technologies like Java and SOA, what is one other industry standard technology it has resources for?\n",
      "F_SCORE: 0.577777777778 [ANS] mozilla firefox [MY_ANS] development ibm office alternative microsoft software [Q] What web browser does the Open Document Format have compatibility with?\n",
      "F_SCORE: 0.65306122449 [ANS] project big green [MY_ANS] re-direction big later per businesses unveiled green across [Q] What is the name of the IBM project that redirected $1 billion each year to increase energy efficiency?\n",
      "F_SCORE: 0.848484848485 [ANS] developerworks [MY_ANS] professionals developerworks [Q] IBM runs what website for software developers?\n",
      "F_SCORE: 1.0 [ANS] 1984 [MY_ANS] 1984 [Q] In what year did IBM add sexual orientation to their nondiscrimination policy?\n",
      "F_SCORE: 0.757894736842 [ANS] computing-tabulating-recording company ( ctr ) [MY_ANS] thomas l. jr. albert j. watson board president williams chairman [Q] What was the name of the company that eventually became IBM?\n",
      "F_SCORE: 0.666666666667 [ANS] endicott [MY_ANS] endicott decades pollution [Q] What location is the birthplace of IBM?\n",
      "F_SCORE: 0.823529411765 [ANS] sexual orientation [MY_ANS] origin age sex [Q] IBM expanded their nondiscrimination policy in 1984 to include what?\n",
      "F_SCORE: 1.0 [ANS] think [MY_ANS] think [Q] IBM employees created a magazine in 1935, what was its name?\n",
      "F_SCORE: 0 [ANS] 2006 [MY_ANS] built low-cost encryption later hardware design microprocessor data ibm [Q] Secure Blue was launched in what year?\n",
      "F_SCORE: 0.5 [ANS] watson [MY_ANS] analytics watson acquisition [Q] What will IBM use to analyze weather and make predictions?\n",
      "F_SCORE: 0.470588235294 [ANS] thomas j. watson [MY_ANS] employee magazine [Q] Who created the IBM name?\n",
      "F_SCORE: 0.75 [ANS] 2012 [MY_ANS] 2005 [Q] In what year did IBM acquire Kenexa?\n",
      "F_SCORE: 1.0 [ANS] 17 [MY_ANS] 17 [Q] How many SmartCamp events does IBM hold worldwide?\n",
      "F_SCORE: 1.0 [ANS] 2012 [MY_ANS] 2012 [Q] What year did the Nintendo Wii U, partly developed by IBM, debut?\n",
      "F_SCORE: 0.47619047619 [ANS] 24.3 % [MY_ANS] 16.5 24.3 9.0 2004 16.8 [Q] IBM's operating margin in 2013 was what?\n",
      "F_SCORE: 0.571428571429 [ANS] ibm research [MY_ANS] worldwide bundled 12 has [Q] Under what name do research laboratories operated by IBM work under?\n",
      "F_SCORE: 0 [ANS] '07 [MY_ANS] 14 [Q] What year did William R. Brody join the IBM Board of Directors?\n",
      "F_SCORE: 0.380952380952 [ANS] 9.0 % [MY_ANS] 16.5 24.3 9.0 2013 16.8 [Q] What were the net profit margins of IBM in 2004?\n",
      "F_SCORE: 0.869565217391 [ANS] international business machines [MY_ANS] completion training schoolhouse employees endicott department education [Q] What does IBM stand for?\n",
      "F_SCORE: 1.0 [ANS] uk [MY_ANS] uk [Q] Charities of which nation benefit from the IBM partnership with Pennies?\n",
      "F_SCORE: 0.24 [ANS] hundred percent club [MY_ANS] yes [Q] In 1925 the first meeting of this group occurred.\n",
      "F_SCORE: 0.428571428571 [ANS] blue gene [MY_ANS] yes [Q] This program was awarded the National Medal of Technology and Innovation.\n",
      "F_SCORE: 0.604651162791 [ANS] eero saarinen [MY_ANS] saarinen company eero design architecture contributions [Q] IBM has worked with architects and designers such as Ludwig Mies van der Rohe, I.M. Pei, and Van der Rohe, name one  more.\n",
      "F_SCORE: 0.912280701754 [ANS] powerpc tri-core processor [MY_ANS] tri-core months ibm powerpc [Q] What kind of processor was in the Xbox 360?\n",
      "F_SCORE: 0.156862745098 [ANS] think [MY_ANS] patterson ctr national register cash flint henry john company [Q] What was the favorite slogan of Thomas J. Watson Sr.?\n",
      "F_SCORE: 0 [ANS] $ 1.2bn [MY_ANS] yes [Q] IBM committed to an expansion totaling this dollar amount in 2014?\n",
      "F_SCORE: 0.878048780488 [ANS] albert l. williams [MY_ANS] thomas l. albert j. watson williams [Q] In 1961 who became the president of IBM?\n",
      "F_SCORE: 0.322580645161 [ANS] sco v. ibm [MY_ANS] proponent initiative linux [Q] What is a notable legal case involving open source and IBM?\n",
      "F_SCORE: 0.717948717949 [ANS] american express [MY_ANS] member directors management board [Q] Kenneth Chenault is affiliated with what company?\n",
      "F_SCORE: 0.594594594595 [ANS] secure blue [MY_ANS] blue secure hardware microprocessor year ibm [Q] What was the design for low cost data encryption named?\n",
      "F_SCORE: 0.393442622951 [ANS] 1990 honor award [MY_ANS] division war van center chicago rohe company museum research honor national der [Q] The 330 North Wabash Building was recognized with what award?\n",
      "F_SCORE: 0.923076923077 [ANS] smarter computing framework [MY_ANS] planet smarter computing [Q] What framework did IBM announce on March 1, 2011?\n",
      "F_SCORE: 0.212121212121 [ANS] an ibm 704 [MY_ANS] ibm intelligence company experience poughkeepsie york laboratory new example [Q] What computer did Arthur L. Samuel program to play checkers in 1957?\n",
      "F_SCORE: 0.285714285714 [ANS] 34 [MY_ANS] 1963 eight 34 [Q] How many inventors were honored at IBM's first Invention Award Dinner?\n",
      "F_SCORE: 0.5 [ANS] 1952 [MY_ANS] 11 1964 one 1952 [Q] In what year was IBM's first equal opportunity policy letter published?\n",
      "F_SCORE: 0.285714285714 [ANS] 435,000 worldwide [MY_ANS] management 's traced back practices roots [Q] How many employees did IBM have in 2012?\n",
      "F_SCORE: 0.4 [ANS] 2013 [MY_ANS] 24.3 9.0 2013 2004 16.8 [Q] IBM's net profit margins were 16.5% in what year?\n",
      "F_SCORE: 0.16091954023 [ANS] lexmark [MY_ANS] product lenovo x86 business companies lexmark lines consulting server computer businesses pwc kenexa spss [Q] IBM span off its printer manufacturer in 1991, what was its name?\n",
      "F_SCORE: 0.969696969697 [ANS] january 29 , 2016 [MY_ANS] january 2016 29 [Q] When did the sale of Weather Company assets close?\n",
      "F_SCORE: 0.424242424242 [ANS] andrew n. liveris [MY_ANS] company management [Q] Who is the most recent member to join the IBM Board of Directors?\n",
      "F_SCORE: 1.0 [ANS] 14 [MY_ANS] 14 [Q] IBM has how many members on its Board of Directors?\n",
      "F_SCORE: 0.21875 [ANS] pennies [MY_ANS] customers ibm pennies debit solution credit way money july card software [Q] What company is known for the electronic charity box?\n",
      "F_SCORE: 1.0 [ANS] 1914 [MY_ANS] 1914 [Q] In what year did Thomas J. Watson, Sr. join CTR?\n",
      "F_SCORE: 0.782608695652 [ANS] the u.s. south [MY_ANS] south [Q] IBM publicized its hiring policy to help negotiations in two states where in the U.S.?\n",
      "F_SCORE: 0.164383561644 [ANS] lenovo [MY_ANS] product printer x86 lexmark lines consulting server lenovo kenexa pwc spss manufacturer [Q] IBM sold its personal computer business to what company?\n",
      "F_SCORE: 0.333333333333 [ANS] the weather company [MY_ANS] yes [Q] IBM acquired digital assets of this company on October 28, 2015.\n",
      "F_SCORE: 0.756756756757 [ANS] charles ranlett flint [MY_ANS] businesses ctr [Q] Name the individual that consolidated the companies that were to become the Computing-Tabulating-Recording Company.\n",
      "F_SCORE: 0.6 [ANS] 78,000 gallons [MY_ANS] 78,000 [Q] Starting in 1980 how many gallons of chemicals did IBM pump into the air?\n",
      "F_SCORE: 0.205882352941 [ANS] pennies [MY_ANS] box customers pennies solution credit way debit july charity card software [Q] Who has IBM partnered with to allow retail shoppers to easily donate money?\n",
      "F_SCORE: 0.540540540541 [ANS] cell be microprocessor [MY_ANS] sony ibm toshiba [Q] Playstation 3 featured which microprocessor?\n",
      "F_SCORE: 0.733333333333 [ANS] five nobel prizes [MY_ANS] six five ten [Q] How many Nobel Prizes have been won by IBM employees?\n",
      "F_SCORE: 0.509803921569 [ANS] fourth largest [MY_ANS] firm fortune capitalization revenue employees number nineteenth u.s. [Q] What was the ranking in terms of market cap for IBM in 2012?\n",
      "F_SCORE: 0.592592592593 [ANS] 31st largest [MY_ANS] company terms [Q] In 2011 Forbes, by revenue, ranked IBM at what rank globally?\n",
      "F_SCORE: 0.836363636364 [ANS] saudi business machines [MY_ANS] bahrain needs arabian-american office business oil machines sbm [Q] What was the eventual name of the company that IBM operated in Saudi Arabia?\n",
      "F_SCORE: 0.25 [ANS] 5 % drop [MY_ANS] 2014 21 [Q] How large of a drop in sales did IBM report for fiscal year 2013?\n",
      "F_SCORE: 1.0 [ANS] 26 million [MY_ANS] 26 million [Q] Records for how many people were maintained by IBM in 1937?\n",
      "F_SCORE: 1.0 [ANS] 1933 [MY_ANS] 1933 [Q] In what year did the companies owned by Computing-Tabulating-Recording Company finally integrate?\n",
      "F_SCORE: 0.773333333333 [ANS] exiting commoditizing markets [MY_ANS] drams stream quality analytics profit pcs revenue drives mix virtualization margins disk data decade solutions [Q] IBM focusing on markets like business continuity, business intelligence, security, and cloud computing is an example of IBM doing what?\n"
     ]
    }
   ],
   "source": [
    "def get_accuracy_train(fname):\n",
    "    counter = float(0)\n",
    "    final_ans = []\n",
    "    with open(fname) as json_data:\n",
    "        infile = json.load(json_data)\n",
    "        length = len(infile)\n",
    "        for dic in infile:\n",
    "            \n",
    "            # training and devel\n",
    "            question = dic['question']\n",
    "            ans = dic['text']\n",
    "            para = dic['answer_paragraph']\n",
    "            docid = dic['docid']    \n",
    "            \n",
    "            query, q_type = extract_query(question)            \n",
    "            my_para_ord, my_para = get_para(docid, query)\n",
    "                        \n",
    "            # my_sent is raw sent in the paragraph\n",
    "            my_sent = get_sent(my_para, query)\n",
    "            my_ans = get_ans(my_sent, query, q_type)\n",
    "            \n",
    "            # calculate precision, recall and accuracy \n",
    "            tp = 0.0\n",
    "            fp = 0.0\n",
    "            fn = 0.0\n",
    "            \n",
    "            for word in ans:\n",
    "                if word in my_ans:\n",
    "                    tp += 1\n",
    "                else:\n",
    "                    fn += 1\n",
    "            for word in my_ans:\n",
    "                if word not in ans:\n",
    "                    fp += 1\n",
    "            \n",
    "            # prec = TP / (TP + FP) \n",
    "            prec = tp / (tp+fp)\n",
    "            # recall = TP / (TP + FN) \n",
    "            recall = tp / (tp+fn)\n",
    "            if prec+recall == 0:\n",
    "                f_score = 0\n",
    "            else:\n",
    "                f_score = 2*prec*recall / (prec+recall)\n",
    "            # end\n",
    "            \n",
    "            rst_dict = {'f_score': f_score,\n",
    "                        'ans': ans, \n",
    "                        'my_ans': my_ans, \n",
    "                        'prec': prec, \n",
    "                        'recall': recall, \n",
    "                        'Q': question, \n",
    "                        'para': para, \n",
    "                        'docid': docid}\n",
    "            \n",
    "            print(\"F_SCORE: \"+ str(f_score) + ' [ANS] ' + ans + ' [MY_ANS] ' + my_ans + ' [Q] ' + question)\n",
    "            \n",
    "            final_ans.append(rst_dict)\n",
    "    output_result_train(final_ans)\n",
    "    \n",
    "\n",
    "develfname = 'project_files/devel_some.json' # Need to change later\n",
    "acc = get_accuracy_train(develfname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# def get_final_result(fname):\n",
    "#     counter = float(0)\n",
    "#     final_ans = []\n",
    "#     with open(fname) as json_data:\n",
    "#         infile = json.load(json_data)\n",
    "#         length = len(infile)\n",
    "#         for dic in infile:  \n",
    "            \n",
    "#             #testing\n",
    "#             question = dic['question']\n",
    "#             docid = dic['docid'] \n",
    "#             # question id\n",
    "#             qid = dic['id']\n",
    "            \n",
    "#             query, q_type = extract_query(question)\n",
    "                        \n",
    "#             my_para_ord, my_para = get_para(docid, query)\n",
    "                        \n",
    "#             # my_sent is raw sent in the paragraph\n",
    "#             my_sent = get_sent(my_para, query)\n",
    "            \n",
    "#             my_ans = get_ans(my_sent, query, q_type)\n",
    "            \n",
    "#             print qid\n",
    "            \n",
    "#             final_ans.append({'id': qid, 'answer': my_ans})\n",
    "            \n",
    "#     output_result(final_ans)\n",
    "    \n",
    "# develfname = 'project_files/testing.json' # Need to change later\n",
    "# get_final_result(develfname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
