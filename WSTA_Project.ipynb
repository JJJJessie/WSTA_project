{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Team Name:     XiaoJiLingGui\n",
    "\n",
    "Kaggle Login:  siqiguo\n",
    "\n",
    "Student Name:  Siqi Guo\n",
    "\n",
    "Student ID:    743053\n",
    "\n",
    "Kaggle Login: \n",
    "\n",
    "Student Name:\n",
    "\n",
    "Student ID: \n",
    "\n",
    "Python version used: 2.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import requests, tarfile\n",
    "import json, string\n",
    "from math import log\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "\n",
    "stopwords = set(nltk.corpus.stopwords.words('english')) # wrap in a set() (see below)\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer() \n",
    "sent_segmenter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "DOCFNAME = 'project_files/documents.json'\n",
    "\n",
    "# More will be added\n",
    "HOW_SUBCLASS = {'many': [[], ['CD']]}\n",
    "\n",
    "QUERY_CLASS = {'who': [['PERSON', 'ORGANIZATION'], ['NNP']], \n",
    "               'when': [['PERSON', 'ORGANIZATION'], ['NNP']]}\n",
    "\n",
    "inverted_index_dict = {} # Store the processed documents' inverted index to improve the effiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def lemmatize_word(word):\n",
    "    word = word.lower()\n",
    "    lemma_n = lemmatizer.lemmatize(word,'n')\n",
    "    lemma_v = lemmatizer.lemmatize(word,'v')\n",
    "    if len(lemma_n) <= len(lemma_v):\n",
    "        return lemma_n\n",
    "    else:\n",
    "        return lemma_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument of type 'builtin_function_or_method' is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-2b3c85f24353>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mdevelfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'project_files/devel.json'\u001b[0m \u001b[0;31m# Need to change later\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpara_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevelfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-83-2b3c85f24353>\u001b[0m in \u001b[0;36mpara_accuracy\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mdocid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'docid'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-694afe9a4c2e>\u001b[0m in \u001b[0;36mextract_query\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mhow_sub\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenized\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhow_index\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mhow_sub\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mHOW_SUBCLASS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mtokenized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhow_index\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mq_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHOW_SUBCLASS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhow_sub\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: argument of type 'builtin_function_or_method' is not iterable"
     ]
    }
   ],
   "source": [
    "def para_accuracy(fname):\n",
    "    counter = float(0)\n",
    "    with open(fname) as json_data:\n",
    "        infile = json.load(json_data)\n",
    "        length = len(infile)\n",
    "        for dic in infile:\n",
    "            question = dic['question']\n",
    "            ans = dic['text']\n",
    "            para = dic['answer_paragraph']\n",
    "            docid = dic['docid']\n",
    "            \n",
    "            query, q_type = extract_query(question)\n",
    "            \n",
    "            result = get_result(docid, query)\n",
    "            if result == para:\n",
    "                counter += 1\n",
    "    return counter/length\n",
    "\n",
    "develfname = 'project_files/devel.json' # Need to change later\n",
    "acc = para_accuracy(develfname)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Select the most relevant paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def extract_query(query):\n",
    "    tokenized = nltk.word_tokenize(query)\n",
    "    \n",
    "    q_type = None\n",
    "    \n",
    "    if 'Who' in tokenized or 'who' in tokenized:\n",
    "        q_type = QUERY_CLASS['who']\n",
    "    elif 'When' in tokenized or 'when' in tokenized:\n",
    "        q_type = QUERY_CLASS['when']\n",
    "    elif 'How' in tokenized or 'how' in tokenized:\n",
    "        \n",
    "        # Since HOW queries have many answer types, furthrer analysis required\n",
    "\n",
    "        how_index = 0\n",
    "        \n",
    "        if 'How' in tokenized:\n",
    "            how_index = tokenized.index('How')\n",
    "        else:\n",
    "            how_index = tokenized.index('how')\n",
    "            \n",
    "        how_sub = tokenized[how_index + 1]\n",
    "        \n",
    "        if how_sub in HOW_SUBCLASS.values:\n",
    "            tokenized.pop(how_index + 1)\n",
    "            q_type = HOW_SUBCLASS[how_sub]\n",
    "        \n",
    "    terms = []\n",
    "    for token in tokenized:\n",
    "        if token not in stopwords and token not in string.punctuation: \n",
    "            terms.append(lemmatize_word(token))\n",
    "    return terms, q_type "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def select_document(fname, docid):\n",
    "    with open(fname) as json_data:\n",
    "        infile = json.load(json_data)\n",
    "        for doc in infile:\n",
    "            if doc['docid'] == docid:\n",
    "                text = doc['text']\n",
    "    return text\n",
    "\n",
    "# def extract_query_terms(query, q_type):\n",
    "#     q_terms = extract_terms(query)\n",
    "#     # Since HOW queries have many answer types, furthrer analysis required\n",
    "#     if q_type == 'HOW':\n",
    "        \n",
    "\n",
    "def parse_paragraphs(text):\n",
    "    identifier = 0\n",
    "    for para in text:\n",
    "        yield (identifier, para)\n",
    "        identifier += 1\n",
    "\n",
    "# def extract_terms(para):\n",
    "#     terms = set()\n",
    "#     for token in nltk.word_tokenize(para):\n",
    "#         if token not in stopwords: # 'in' and 'not in' opera]tions are much faster over sets that lists\n",
    "#             terms.add(lemmatize_word(token))\n",
    "#     return terms\n",
    "\n",
    "def extract_term_freqs(para):\n",
    "    tfs = Counter()\n",
    "    for token in nltk.word_tokenize(para):\n",
    "        if token not in stopwords and token not in string.punctuation: # 'in' and 'not in' operations are much faster over sets that lists\n",
    "            tfs[lemmatize_word(token)] += 1\n",
    "    return tfs\n",
    "\n",
    "def compute_para_freqs(para_term_freqs):\n",
    "    pfs = Counter()\n",
    "    for tfs in para_term_freqs.values():\n",
    "        for term in tfs.keys():\n",
    "            pfs[term] += 1\n",
    "    return pfs\n",
    "\n",
    "def get_index(para_term_freqs, para_freqs, M):\n",
    "    vsm_inverted_index = defaultdict(list)\n",
    "    for pid, term_freqs in para_term_freqs.items():\n",
    "        N = sum(term_freqs.values())\n",
    "        length = 0\n",
    "\n",
    "        # find tf*idf values and accumulate sum of squares \n",
    "        tfidf_values = []\n",
    "        for term, count in term_freqs.items():\n",
    "            tfidf = float(count) / N * log(M / float(para_freqs[term]))\n",
    "            tfidf_values.append((term, tfidf))\n",
    "            length += tfidf ** 2\n",
    "\n",
    "        # normalise documents by length and insert into index\n",
    "        length = length ** 0.5\n",
    "        for term, tfidf in tfidf_values:\n",
    "            # note the inversion of the indexing, to be term -> (doc_id, score)\n",
    "            vsm_inverted_index[term].append([pid, tfidf / length])\n",
    "\n",
    "    # ensure posting lists are in sorted order (less important here cf above)\n",
    "    for term, pids in vsm_inverted_index.items():\n",
    "        pids.sort()\n",
    "    return vsm_inverted_index\n",
    "    \n",
    "def query_vsm(query, index, k=4): # Only return the most revelant paragraph\n",
    "    accumulator = Counter()\n",
    "    for term in query:\n",
    "        postings = index[term]\n",
    "        for pid, weight in postings:\n",
    "            accumulator[pid] += weight\n",
    "    return accumulator.most_common(k)\n",
    "\n",
    "def get_freq_dict(raw_paras):\n",
    "#     paras = {}\n",
    "#     for pid, p in raw_paras:\n",
    "#         terms = extract_terms(p)\n",
    "#         paras[pid] = terms\n",
    "\n",
    "    para_term_freqs = {}\n",
    "    for pid, para in raw_paras:\n",
    "        term_freqs = extract_term_freqs(para)\n",
    "        para_term_freqs[pid] = term_freqs\n",
    "    M = len(para_term_freqs)\n",
    "    return para_term_freqs, M\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_result(docid, query):\n",
    "    text = select_document(DOCFNAME, docid)\n",
    "    raw_paras = list(parse_paragraphs(text))\n",
    "    if docid not in inverted_index_dict.keys(): # check whether the document has been processed before\n",
    "#         text = select_document(DOCFNAME, docid)\n",
    "#         raw_paras = list(parse_paragraphs(text))\n",
    "\n",
    "        para_term_freqs, M = get_freq_dict(raw_paras)\n",
    "        para_freqs = compute_para_freqs(para_term_freqs)\n",
    "        inverted_index = get_index(para_term_freqs, para_freqs, M)\n",
    "        inverted_index_dict[docid] = inverted_index\n",
    "    else:\n",
    "        inverted_index = inverted_index_dict[docid]\n",
    "    results = query_vsm(query, inverted_index)\n",
    "    return raw_paras[results[0][0]][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sent(result_para, query):\n",
    "    sentences = sent_segmenter.tokenize(result_para)\n",
    "    raw_sents = list(parse_paragraphs(sentences))\n",
    "    sent_term_freqs, M = get_freq_dict(raw_sents)\n",
    "    sent_freqs = compute_para_freqs(sent_term_freqs)\n",
    "    results = query_vsm(query, get_index(sent_term_freqs, sent_freqs, M))\n",
    "    return raw_sents[results[0][0]][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was eventually called the photon.\n"
     ]
    }
   ],
   "source": [
    "result_para = get_result(0, ['eventually', 'call', 'photon'])\n",
    "# print result_para\n",
    "sent = get_sent(result_para, ['eventually', 'call', 'photon'])\n",
    "print sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
