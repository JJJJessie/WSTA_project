{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Team Name:     XiaoJiLingGui\n",
    "\n",
    "Kaggle Login:  siqiguo\n",
    "\n",
    "Student Name:  Siqi Guo\n",
    "\n",
    "Student ID:    743053\n",
    "\n",
    "Kaggle Login: yifan66\n",
    "\n",
    "Student Name: Yifan Wang\n",
    "\n",
    "Student ID: 784386\n",
    "\n",
    "Python version used: 2.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import requests, tarfile\n",
    "import json, string\n",
    "from math import log\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "\n",
    "stopwords = set(nltk.corpus.stopwords.words('english')) # wrap in a set() (see below)\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer() \n",
    "sent_segmenter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "DOCFNAME = 'project_files/documents.json'\n",
    "\n",
    "WHAT = 'what'\n",
    "WHO = 'who'\n",
    "HOW = 'how'\n",
    "WHERE = 'where'\n",
    "WHEN = 'when'\n",
    "WHICH = 'which'\n",
    "NAME = 'name'\n",
    "WHY = 'why'\n",
    "WHOM = 'whom'\n",
    "BINARY = 'binary'\n",
    "POLAR = 'polar'\n",
    "\n",
    "HOW_SUBCLASS = {'many': [[], ['CD']], \n",
    "                'long': [['DURATION'], ['CD']], \n",
    "                'much': [['MONEY'], ['CD', 'FW', 'NN']], \n",
    "                'far': [[], ['CD']], \n",
    "                'tall': [[], ['CD']], \n",
    "                'rich': [['MONEY'], ['CD']], \n",
    "                'large': [[], ['CD']], \n",
    "                }\n",
    "\n",
    "QUERY_CLASS = {WHAT: [[], ['NN']], \n",
    "               WHO: [['PERSON', 'ORGANIZATION'], ['NN']], \n",
    "               WHERE: [['LOCATION'], ['NN']],\n",
    "               WHEN: [['DATE', 'TIME'], ['CD', 'NN']],\n",
    "               WHICH: [['PERSON', 'LOCATION', 'DATE', 'TIME', 'ORGANIZATION'], ['NN']],\n",
    "               NAME: [['PERSON', 'LOCATION', 'ORGANIZATION'], ['NN']],\n",
    "               #'why': reason, find \"reason word\" \n",
    "               WHOM: [['PERSON', 'ORGANIZATION'], ['NN']], \n",
    "               \n",
    "               # LOCATION, sometimes 'CD' also included as street number, unit number \n",
    "               # what: if and not in query terms, then \n",
    "              \n",
    "              }\n",
    "\n",
    "inverted_index_dict = {} # Store the processed documents' inverted index to improve the effiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def lemmatize_word(word):\n",
    "    word = word.lower()\n",
    "    lemma_n = lemmatizer.lemmatize(word,'n')\n",
    "    lemma_v = lemmatizer.lemmatize(word,'v')\n",
    "    \n",
    "    # If both change, return the shorter one\n",
    "    # If only one change, return the changed one\n",
    "    # If neither change, return the original word\n",
    "    \n",
    "    if lemma_n != word and lemma_v != word:    \n",
    "        if len(lemma_n) < len(lemma_v):\n",
    "            return lemma_n\n",
    "        else:\n",
    "            return lemma_v\n",
    "    elif lemma_n == word and lemma_v == word:\n",
    "        return word\n",
    "    elif lemma_n != word:\n",
    "        return lemma_n\n",
    "    elif lemma_v != word:\n",
    "        return lemma_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Select the most relevant paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def extract_query(query):\n",
    "    tokenized = nltk.word_tokenize(query)\n",
    "    tokenized = [x.lower() for x in tokenized]\n",
    "    q_type = None\n",
    "    \n",
    "    if WHAT in tokenized:\n",
    "        q_type = WHAT\n",
    "                \n",
    "    elif WHO in tokenized:\n",
    "        q_type = WHO\n",
    "        \n",
    "    elif HOW in tokenized:  \n",
    "        # Since HOW queries have many answer types, furthrer analysis required\n",
    "        how_index = tokenized.index(HOW)\n",
    "            \n",
    "        how_sub = tokenized[how_index + 1]\n",
    "        \n",
    "        if how_sub in HOW_SUBCLASS.keys():\n",
    "            tokenized.pop(how_index + 1)\n",
    "            q_type = how_sub\n",
    "        else:\n",
    "            q_type = HOW\n",
    "\n",
    "    elif WHERE in tokenized:\n",
    "        q_type = WHERE\n",
    "     \n",
    "    elif WHEN in tokenized:\n",
    "        # Use SUTime\n",
    "        q_type = WHEN\n",
    "    \n",
    "    elif WHICH in tokenized:\n",
    "        q_type = WHICH\n",
    "        \n",
    "    elif WHY in tokenized:\n",
    "        q_type = WHY\n",
    "    \n",
    "    elif WHOM in tokenized:\n",
    "        q_type = WHOM\n",
    "    \n",
    "    elif NAME in tokenized:\n",
    "        q_type = NAME\n",
    "    else:\n",
    "        if 'or' in tokenized:\n",
    "            q_type = BINARY\n",
    "        else:\n",
    "            q_type = POLAR\n",
    "        \n",
    "    terms = []\n",
    "    for token in tokenized:\n",
    "        if token not in stopwords and token not in string.punctuation: \n",
    "            terms.append(lemmatize_word(token))\n",
    "            \n",
    "    return terms, q_type "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def select_document(fname, docid):\n",
    "    with open(fname) as json_data:\n",
    "        infile = json.load(json_data)\n",
    "        for doc in infile:\n",
    "            if doc['docid'] == docid:\n",
    "                text = doc['text']\n",
    "    return text\n",
    "\n",
    "# def extract_query_terms(query, q_type):\n",
    "#     q_terms = extract_terms(query)\n",
    "#     # Since HOW queries have many answer types, furthrer analysis required\n",
    "#     if q_type == 'HOW':\n",
    "        \n",
    "\n",
    "def parse_paragraphs(text):\n",
    "    identifier = 0\n",
    "    for para in text:\n",
    "        yield (identifier, para)\n",
    "        identifier += 1\n",
    "\n",
    "# def extract_terms(para):\n",
    "#     terms = set()\n",
    "#     for token in nltk.word_tokenize(para):\n",
    "#         if token not in stopwords: # 'in' and 'not in' opera]tions are much faster over sets that lists\n",
    "#             terms.add(lemmatize_word(token))\n",
    "#     return terms\n",
    "\n",
    "def extract_term_freqs(para):\n",
    "    tfs = Counter()\n",
    "    for token in nltk.word_tokenize(para):\n",
    "        if token not in stopwords and token not in string.punctuation: # 'in' and 'not in' operations are much faster over sets that lists\n",
    "            tfs[lemmatize_word(token)] += 1\n",
    "    return tfs\n",
    "\n",
    "def compute_para_freqs(para_term_freqs):\n",
    "    pfs = Counter()\n",
    "    for tfs in para_term_freqs.values():\n",
    "        for term in tfs.keys():\n",
    "            pfs[term] += 1\n",
    "    return pfs\n",
    "\n",
    "def get_index(para_term_freqs, para_freqs, M):\n",
    "    vsm_inverted_index = defaultdict(list)\n",
    "    for pid, term_freqs in para_term_freqs.items():\n",
    "        N = sum(term_freqs.values())\n",
    "        length = 0\n",
    "\n",
    "        # find tf*idf values and accumulate sum of squares \n",
    "        tfidf_values = []\n",
    "        for term, count in term_freqs.items():\n",
    "            tfidf = float(count) / N * log(M / float(para_freqs[term]))\n",
    "            tfidf_values.append((term, tfidf))\n",
    "            length += tfidf ** 2\n",
    "\n",
    "        # normalise documents by length and insert into index\n",
    "        length = length ** 0.5\n",
    "        for term, tfidf in tfidf_values:\n",
    "            # note the inversion of the indexing, to be term -> (doc_id, score)\n",
    "            vsm_inverted_index[term].append([pid, tfidf / length])\n",
    "\n",
    "    # ensure posting lists are in sorted order (less important here cf above)\n",
    "    for term, pids in vsm_inverted_index.items():\n",
    "        pids.sort()\n",
    "    return vsm_inverted_index\n",
    "    \n",
    "def query_vsm(query, index, k=4): # Only return the most revelant paragraph\n",
    "    accumulator = Counter()\n",
    "    for term in query:\n",
    "        postings = index[term]\n",
    "        for pid, weight in postings:\n",
    "            accumulator[pid] += weight\n",
    "    return accumulator.most_common(k)\n",
    "\n",
    "def get_freq_dict(raw_paras):\n",
    "#     paras = {}\n",
    "#     for pid, p in raw_paras:\n",
    "#         terms = extract_terms(p)\n",
    "#         paras[pid] = terms\n",
    "\n",
    "    para_term_freqs = {}\n",
    "    for pid, para in raw_paras:\n",
    "        term_freqs = extract_term_freqs(para)\n",
    "        para_term_freqs[pid] = term_freqs\n",
    "    M = len(para_term_freqs)\n",
    "    return para_term_freqs, M\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_para(docid, query):\n",
    "    text = select_document(DOCFNAME, docid)\n",
    "    raw_paras = list(parse_paragraphs(text))\n",
    "    if docid not in inverted_index_dict.keys(): # check whether the document has been processed before\n",
    "#         text = select_document(DOCFNAME, docid)\n",
    "#         raw_paras = list(parse_paragraphs(text))\n",
    "\n",
    "        para_term_freqs, M = get_freq_dict(raw_paras)\n",
    "        para_freqs = compute_para_freqs(para_term_freqs)\n",
    "        inverted_index = get_index(para_term_freqs, para_freqs, M)\n",
    "        inverted_index_dict[docid] = inverted_index\n",
    "    else:\n",
    "        inverted_index = inverted_index_dict[docid]\n",
    "    results = query_vsm(query, inverted_index)\n",
    "    print results\n",
    "    return raw_paras[results[0][0]][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_sent(result_para, query):\n",
    "    sentences = sent_segmenter.tokenize(result_para)\n",
    "    raw_sents = list(parse_paragraphs(sentences))\n",
    "    sent_term_freqs, M = get_freq_dict(raw_sents)\n",
    "    sent_freqs = compute_para_freqs(sent_term_freqs)\n",
    "    results = query_vsm(query, get_index(sent_term_freqs, sent_freqs, M))\n",
    "    return raw_sents[results[0][0]][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the literal  [u'literal', u'translation', u'theokotos']\n",
      "[]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-6efa70db752d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mdevelfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'project_files/mytest.json'\u001b[0m \u001b[0;31m# Need to change later\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevelfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-32-6efa70db752d>\u001b[0m in \u001b[0;36mget_accuracy\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mprint\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_para\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mpara\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-0fb43bc9f198>\u001b[0m in \u001b[0;36mget_para\u001b[0;34m(docid, query)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_vsm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minverted_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mraw_paras\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "def get_accuracy(fname):\n",
    "    counter = float(0)\n",
    "    with open(fname) as json_data:\n",
    "        infile = json.load(json_data)\n",
    "        length = len(infile)\n",
    "        for dic in infile:\n",
    "            question = dic['question']\n",
    "            ans = dic['text']\n",
    "            para = dic['answer_paragraph']\n",
    "            docid = dic['docid']    \n",
    "            \n",
    "            query, q_type = extract_query(question)\n",
    "            \n",
    "            print question[:20], query\n",
    "            \n",
    "            result = get_para(docid, query)\n",
    "            \n",
    "            if result == para:\n",
    "                counter += 1\n",
    "    return counter/length\n",
    "\n",
    "develfname = 'project_files/mytest.json' # Need to change later\n",
    "acc = get_accuracy(develfname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# result_para = get_para(0, ['eventually', 'call', 'photon'])\n",
    "# # print result_para\n",
    "# sent = get_sent(result_para, ['eventually', 'call', 'photon'])\n",
    "# print sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
