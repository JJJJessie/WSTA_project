{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Team Name:     XiaoJiLingGui\n",
    "\n",
    "Kaggle Login:  siqiguo\n",
    "\n",
    "Student Name:  Siqi Guo\n",
    "\n",
    "Student ID:    743053\n",
    "\n",
    "Kaggle Login: yifan66\n",
    "\n",
    "Student Name: Yifan Wang\n",
    "\n",
    "Student ID: 784386\n",
    "\n",
    "Python version used: 2.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import json, string\n",
    "from math import log\n",
    "from collections import defaultdict, Counter\n",
    "import csv\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tag.stanford import StanfordPOSTagger\n",
    "\n",
    "stopwords = set(nltk.corpus.stopwords.words('english')) # wrap in a set() (see below)\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer() \n",
    "sent_segmenter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "DOCFNAME = 'project_files/documents.json'\n",
    "\n",
    "WHAT = 'what'\n",
    "WHO = 'who'\n",
    "HOW = 'how'\n",
    "WHERE = 'where'\n",
    "WHEN = 'when'\n",
    "WHICH = 'which'\n",
    "NAME = 'name'\n",
    "WHY = 'why'\n",
    "WHOM = 'whom'\n",
    "BINARY = 'binary'\n",
    "POLAR = 'polar'\n",
    "\n",
    "PUNCTUATION = string.punctuation.translate(None, '$')\n",
    "\n",
    "NER = 0\n",
    "\n",
    "POS = 1\n",
    "\n",
    "NOT_WORDS = ['not', \"n't\"]\n",
    "\n",
    "REASON_WORDS = ['becauce', 'since', 'due to', 'thanks to', 'owing to', \n",
    "                'on account of', 'result', 'for']\n",
    "\n",
    "HOW_SUBCLASS = {'many': [[], ['CD', 'FW']], \n",
    "                'long': [['DURATION'], ['CD', 'FW']], \n",
    "                'much': [['MONEY'], ['$', 'CD', 'FW', 'NN']], \n",
    "                'far': [[], ['CD', 'FW']], \n",
    "                'tall': [[], ['CD', 'FW']], \n",
    "                'rich': [['MONEY'], ['$', 'CD', 'FW']], \n",
    "                'large': [[], ['CD', 'FW']], \n",
    "                }\n",
    "\n",
    "QUERY_CLASS = {WHAT: [[], ['NN', 'FW']],\n",
    "               # To optimise, not include WHO - 'ORGANIZATION' in the tag\n",
    "               WHO: [['PERSON'], ['NN', 'FW']], \n",
    "               WHERE: [['LOCATION'], ['NN', 'FW']],\n",
    "               WHEN: [['DATE', 'TIME'], ['CD', 'NN', 'FW']],\n",
    "               WHICH: [['PERSON', 'LOCATION', 'DATE', 'TIME', 'ORGANIZATION'], ['NN', 'FW']],\n",
    "               NAME: [['PERSON', 'LOCATION', 'ORGANIZATION'], ['NN', 'FW']],\n",
    "               #'why': reason, find \"reason word\" \n",
    "               WHOM: [['PERSON'], ['NN', 'FW']],\n",
    "               \n",
    "               # LOCATION, sometimes 'CD' also included as street number, unit number \n",
    "               # what: if and not in query terms, then \n",
    "              \n",
    "              }\n",
    "\n",
    "inverted_index_dict = {} # Store the processed documents' inverted index to improve the effiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ner_dir = 'stanford-ner-2018-02-27/'\n",
    "ner_jarfile = ner_dir + 'stanford-ner.jar'\n",
    "ner_modelfile = ner_dir + 'classifiers/english.all.3class.distsim.crf.ser.gz'\n",
    "ner_tagger = StanfordNERTagger(model_filename=ner_modelfile, path_to_jar=ner_jarfile)\n",
    "\n",
    "pos_dir = 'stanford-postagger-2018-02-27/'\n",
    "pos_modelfile = pos_dir + 'models/english-bidirectional-distsim.tagger'\n",
    "pos_jarfile = pos_dir + 'stanford-postagger.jar'\n",
    "pos_tagger = StanfordPOSTagger(model_filename=pos_modelfile, path_to_jar=pos_jarfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def lemmatize_word(word):\n",
    "    word = word.lower()\n",
    "#     print word\n",
    "    adj_word = get_adj(word)\n",
    "    if adj_word != None:\n",
    "        return adj_word\n",
    "    lemma_n = lemmatizer.lemmatize(word, 'n')\n",
    "    lemma_v = lemmatizer.lemmatize(word, 'v')\n",
    "    \n",
    "    # If both change, return the shorter one\n",
    "    # If only one change, return the changed one\n",
    "    # If neither change, return the original word\n",
    "    \n",
    "    if lemma_n != word and lemma_v != word:    \n",
    "        if len(lemma_n) < len(lemma_v):\n",
    "            return lemma_n\n",
    "        else:\n",
    "            return lemma_v\n",
    "    elif lemma_n == word and lemma_v == word:\n",
    "        return word\n",
    "    elif lemma_n != word:\n",
    "        return lemma_n\n",
    "    elif lemma_v != word:\n",
    "        return lemma_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_adj(word):\n",
    "    for syn in wn.synsets(word):\n",
    "        syn_split = syn.name().split('.')\n",
    "        if syn_split[0] == word and syn_split[1] == 'r':\n",
    "            for lemmas in syn.lemmas(): # all possible lemmas\n",
    "                if lemmas.name() == word:\n",
    "                    if lemmas.pertainyms() == []:\n",
    "                        return None\n",
    "                    return lemmas.pertainyms()[0].name()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# lemmatize_word('lakes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Select the most relevant paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def extract_query(query):\n",
    "    tokenized = nltk.word_tokenize(query)\n",
    "    tokenized = [x.lower() for x in tokenized]\n",
    "    q_type = None\n",
    "    \n",
    "    if WHAT in tokenized:\n",
    "        q_type = WHAT\n",
    "                \n",
    "    elif WHO in tokenized:\n",
    "        q_type = WHO\n",
    "        \n",
    "    elif HOW in tokenized: \n",
    "        q_type = HOW\n",
    "        # Since HOW queries have many answer types, furthrer analysis required\n",
    "#         how_index = tokenized.index(HOW)\n",
    "            \n",
    "#         how_sub = tokenized[how_index + 1]\n",
    "        \n",
    "#         if how_sub in HOW_SUBCLASS.keys():\n",
    "#             tokenized.pop(how_index + 1)\n",
    "#             q_type = how_sub\n",
    "#         else:\n",
    "\n",
    "    elif WHERE in tokenized:\n",
    "        q_type = WHERE\n",
    "     \n",
    "    elif WHEN in tokenized:\n",
    "        # Use SUTime\n",
    "        q_type = WHEN\n",
    "    \n",
    "    elif WHICH in tokenized:\n",
    "        q_type = WHICH\n",
    "        \n",
    "    elif WHY in tokenized:\n",
    "        q_type = WHY\n",
    "    \n",
    "    elif WHOM in tokenized:\n",
    "        q_type = WHOM\n",
    "    \n",
    "    elif NAME in tokenized:\n",
    "        q_type = NAME\n",
    "    else:\n",
    "        if 'or' in tokenized:\n",
    "            q_type = BINARY\n",
    "        else:\n",
    "            q_type = POLAR\n",
    "        \n",
    "    terms = []\n",
    "    for token in tokenized:\n",
    "        if token not in stopwords and token not in PUNCTUATION: \n",
    "            terms.append(lemmatize_word(token))\n",
    "            \n",
    "    return terms, q_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def select_document(fname, docid):\n",
    "    with open(fname) as json_data:\n",
    "        infile = json.load(json_data)\n",
    "        for doc in infile:\n",
    "            if doc['docid'] == docid:\n",
    "                text = doc['text']\n",
    "    return text\n",
    "        \n",
    "\n",
    "def parse_paragraphs(text):\n",
    "    identifier = 0\n",
    "    for para in text:\n",
    "        yield (identifier, para)\n",
    "        identifier += 1\n",
    "\n",
    "def extract_term_freqs(para):\n",
    "    tfs = Counter()\n",
    "    for token in nltk.word_tokenize(para):\n",
    "        if token not in stopwords and token not in PUNCTUATION: # 'in' and 'not in' operations are much faster over sets that lists\n",
    "            tfs[lemmatize_word(token)] += 1\n",
    "    return tfs\n",
    "\n",
    "def compute_para_freqs(para_term_freqs):\n",
    "    pfs = Counter()\n",
    "    for tfs in para_term_freqs.values():\n",
    "        for term in tfs.keys():\n",
    "            pfs[term] += 1\n",
    "    return pfs\n",
    "\n",
    "def get_index(para_term_freqs, para_freqs, M):\n",
    "    vsm_inverted_index = defaultdict(list)\n",
    "    for pid, term_freqs in para_term_freqs.items():\n",
    "        N = sum(term_freqs.values())\n",
    "        length = 0\n",
    "\n",
    "        # find tf*idf values and accumulate sum of squares \n",
    "        tfidf_values = []\n",
    "        for term, count in term_freqs.items():\n",
    "            tfidf = float(count) / N * log(M / float(para_freqs[term]))\n",
    "            tfidf_values.append((term, tfidf))\n",
    "            length += tfidf ** 2\n",
    "\n",
    "        # normalise documents by length and insert into index\n",
    "        length = length ** 0.5\n",
    "        for term, tfidf in tfidf_values:\n",
    "            # note the inversion of the indexing, to be term -> (doc_id, score)\n",
    "            vsm_inverted_index[term].append([pid, tfidf / length])\n",
    "\n",
    "    # ensure posting lists are in sorted order (less important here cf above)\n",
    "    for term, pids in vsm_inverted_index.items():\n",
    "        pids.sort()\n",
    "    return vsm_inverted_index\n",
    "    \n",
    "def query_vsm(query, index, k=4): # Only return the most revelant paragraph\n",
    "    accumulator = Counter()\n",
    "    for term in query:\n",
    "        postings = index[term]\n",
    "        for pid, weight in postings:\n",
    "            accumulator[pid] += weight\n",
    "    return accumulator.most_common(k)\n",
    "\n",
    "def get_freq_dict(raw_paras):\n",
    "    para_term_freqs = {}\n",
    "    for pid, para in raw_paras:\n",
    "        term_freqs = extract_term_freqs(para)\n",
    "        para_term_freqs[pid] = term_freqs\n",
    "    M = len(para_term_freqs)\n",
    "    return para_term_freqs, M\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_para(docid, query):\n",
    "    text = select_document(DOCFNAME, docid)\n",
    "    raw_paras = list(parse_paragraphs(text))\n",
    "    if docid not in inverted_index_dict.keys(): # check whether the document has been processed before\n",
    "#         text = select_document(DOCFNAME, docid)\n",
    "#         raw_paras = list(parse_paragraphs(text))\n",
    "\n",
    "        para_term_freqs, M = get_freq_dict(raw_paras)\n",
    "        para_freqs = compute_para_freqs(para_term_freqs)\n",
    "        inverted_index = get_index(para_term_freqs, para_freqs, M)\n",
    "        inverted_index_dict[docid] = inverted_index\n",
    "    else:\n",
    "        inverted_index = inverted_index_dict[docid]\n",
    "    results = query_vsm(query, inverted_index)\n",
    "    if results == []:\n",
    "        return 0, raw_paras[0][1]\n",
    "    else:\n",
    "        return results[0][0], raw_paras[results[0][0]][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_ans(my_sent, query, q_type):\n",
    "    \n",
    "    ans = ''\n",
    "    \n",
    "    if q_type == WHAT:\n",
    "        ans = get_what_ans(my_sent, query)\n",
    "    elif q_type == WHO:\n",
    "        ans = get_who_ans(my_sent, query)\n",
    "    elif q_type == HOW:  \n",
    "        ans = get_how_ans(my_sent, query)\n",
    "    elif q_type == WHERE:\n",
    "        ans = get_where_ans(my_sent, query)\n",
    "    elif q_type == WHEN:\n",
    "        ans = get_when_ans(my_sent, query)\n",
    "    elif q_type == WHICH:\n",
    "        ans = get_which_ans(my_sent, query)\n",
    "    elif q_type == WHY:\n",
    "        ans = get_why_ans(my_sent, query)\n",
    "    elif q_type == WHOM:\n",
    "        ans = get_whom_ans(my_sent, query)\n",
    "    elif q_type == NAME:\n",
    "        ans = get_name_ans(my_sent, query)\n",
    "    elif q_type == BINARY:\n",
    "        ans = get_binary_ans(my_sent, query)\n",
    "    elif q_type == POLAR:\n",
    "        ans = get_polar_ans(my_sent, query)  \n",
    "        \n",
    "    if ans == '':\n",
    "        return my_sent\n",
    "    else:\n",
    "        return ans\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_what_ans(my_sent, query):\n",
    "    sent_splt = nltk.word_tokenize(my_sent)\n",
    "    sent_splt_lemma = [lemmatize_word(x) for x in sent_splt]\n",
    "    \n",
    "    pos_candidate = tag_sent(sent_splt, sent_splt_lemma, query, POS)\n",
    "    \n",
    "    pos_tags = QUERY_CLASS[WHAT][POS]\n",
    "    \n",
    "    # special case: \"what color\"\n",
    "    if 'color' in query:\n",
    "        pos_tags.append('JJ')\n",
    "\n",
    "    # special case: \"what value\"\n",
    "    if 'value' in query:\n",
    "        pos_tags.append('CD')\n",
    "    \n",
    "    ans_candidate = get_ans_candidate(pos_candidate, pos_tags, POS)\n",
    "\n",
    "    return get_ans_string(sent_splt, ans_candidate)\n",
    "    \n",
    "def get_who_ans(my_sent, query):\n",
    "    sent_splt = nltk.word_tokenize(my_sent)\n",
    "    sent_splt_lemma = [lemmatize_word(x) for x in sent_splt]\n",
    "    \n",
    "    ner_candidate = tag_sent(sent_splt, sent_splt_lemma, query, NER)\n",
    "    \n",
    "    ner_tags = QUERY_CLASS[WHO][NER]\n",
    "    \n",
    "    ans_candidate = get_ans_candidate(ner_candidate, ner_tags, NER)\n",
    "    \n",
    "    if ans_candidate == []:\n",
    "        pos_candidate = tag_sent(sent_splt, sent_splt_lemma, query, POS)\n",
    "        pos_tags = QUERY_CLASS[WHO][POS]\n",
    "        ans_candidate = get_ans_candidate(pos_candidate, pos_tags, POS)\n",
    "        \n",
    "    return get_ans_string(sent_splt, ans_candidate)\n",
    "    \n",
    "def get_where_ans(my_sent, query):\n",
    "    sent_splt = nltk.word_tokenize(my_sent)\n",
    "    sent_splt_lemma = [lemmatize_word(x) for x in sent_splt]\n",
    "    \n",
    "    ner_candidate = tag_sent(sent_splt, sent_splt_lemma, query, NER)\n",
    "    \n",
    "    ner_tags = QUERY_CLASS[WHERE][NER]\n",
    "    \n",
    "    ans_candidate = get_ans_candidate(ner_candidate, ner_tags, NER)\n",
    "    \n",
    "    if ans_candidate == []:\n",
    "        pos_candidate = tag_sent(sent_splt, sent_splt_lemma, query, POS)\n",
    "        pos_tags = QUERY_CLASS[WHERE][POS]\n",
    "        ans_candidate = get_ans_candidate(pos_candidate, pos_tags, POS)\n",
    "        \n",
    "    return get_ans_string(sent_splt, ans_candidate)\n",
    "    \n",
    "def get_when_ans(my_sent, query):\n",
    "    sent_splt = nltk.word_tokenize(my_sent)\n",
    "    sent_splt_lemma = [lemmatize_word(x) for x in sent_splt]\n",
    "    \n",
    "    ner_candidate = tag_sent(sent_splt, sent_splt_lemma, query, NER)\n",
    "    \n",
    "    ner_tags = QUERY_CLASS[WHEN][NER]\n",
    "    \n",
    "    ans_candidate = get_ans_candidate(ner_candidate, ner_tags, NER)\n",
    "    \n",
    "    if ans_candidate == []:\n",
    "        pos_candidate = tag_sent(sent_splt, sent_splt_lemma, query, POS)\n",
    "        pos_tags = QUERY_CLASS[WHEN][POS]\n",
    "        ans_candidate = get_ans_candidate(pos_candidate, pos_tags, POS)\n",
    "        \n",
    "    return get_ans_string(sent_splt, ans_candidate)\n",
    "    \n",
    "def get_which_ans(my_sent, query):\n",
    "    sent_splt = nltk.word_tokenize(my_sent)\n",
    "    sent_splt_lemma = [lemmatize_word(x) for x in sent_splt]\n",
    "    \n",
    "    ner_candidate = tag_sent(sent_splt, sent_splt_lemma, query, NER)\n",
    "    \n",
    "    ner_tags = QUERY_CLASS[WHICH][NER]\n",
    "    \n",
    "    ans_candidate = get_ans_candidate(ner_candidate, ner_tags, NER)\n",
    "    \n",
    "    if ans_candidate == []:\n",
    "        pos_candidate = tag_sent(sent_splt, sent_splt_lemma, query, POS)\n",
    "        pos_tags = QUERY_CLASS[WHICH][POS]\n",
    "        ans_candidate = get_ans_candidate(pos_candidate, pos_tags, POS)\n",
    "        \n",
    "    return get_ans_string(sent_splt, ans_candidate)\n",
    "\n",
    "    \n",
    "def get_why_ans(my_sent, query):\n",
    "    \n",
    "    for word in REASON_WORDS:\n",
    "        if word in my_sent:\n",
    "            my_sent = my_sent.split(word,1)[1] \n",
    "            \n",
    "    sent_splt = nltk.word_tokenize(my_sent)\n",
    "    \n",
    "    # special case - \"on one's account\" / \"on this account\"\n",
    "    if ('account' in sent_splt and 'on' in sent_splt \n",
    "        and (sent_splt.index('account') == sent_splt.index('on')+2 or \n",
    "             sent_splt.index('account') == sent_splt.index('on')+3)):\n",
    "        \n",
    "        sent_splt.remove('account')\n",
    "        sent_splt.remove('on')\n",
    "        \n",
    "    \n",
    "    sent_splt_lemma = [lemmatize_word(x) for x in sent_splt]\n",
    "    \n",
    "    ans_candidate = []\n",
    "    for i in range(len(sent_splt_lemma)):\n",
    "        word = sent_splt_lemma[i]\n",
    "        if (word not in query and word not in stopwords and \n",
    "            word not in PUNCTUATION):\n",
    "            ans_candidate.append(i)\n",
    "\n",
    "    return get_ans_string(sent_splt, ans_candidate)\n",
    "    \n",
    "def get_whom_ans(my_sent, query):\n",
    "    sent_splt = nltk.word_tokenize(my_sent)\n",
    "    sent_splt_lemma = [lemmatize_word(x) for x in sent_splt]\n",
    "    pos_candidate = tag_sent(sent_splt, sent_splt_lemma, query, POS)\n",
    "\n",
    "    if pos_candidate.has_key('IN'):\n",
    "        prep_index = pos_candidate['IN'].sort()[0]\n",
    "        sent_splt = sent_splt[prep_index+1:]\n",
    "        sent_splt_lemma = sent_splt_lemma[prep_index+1:]\n",
    "        pos_candidate = tag_sent(sent_splt, sent_splt_lemma, query, POS)\n",
    "\n",
    "    ner_candidate = tag_sent(sent_splt, sent_splt_lemma, query, NER)\n",
    "    \n",
    "    ner_tags = QUERY_CLASS[WHOM][NER]\n",
    "    \n",
    "    ans_candidate = get_ans_candidate(ner_candidate, ner_tags, NER)\n",
    "    \n",
    "    if ans_candidate == []:\n",
    "        pos_tags = QUERY_CLASS[WHOM][POS]\n",
    "        ans_candidate = get_ans_candidate(pos_candidate, pos_tags, POS)\n",
    "        \n",
    "    return get_ans_string(sent_splt, ans_candidate)\n",
    "    \n",
    "def get_name_ans(my_sent, query):\n",
    "    sent_splt = nltk.word_tokenize(my_sent)\n",
    "    sent_splt_lemma = [lemmatize_word(x) for x in sent_splt]\n",
    "    \n",
    "    pos_candidate = tag_sent(sent_splt, sent_splt_lemma, query, POS)\n",
    "    \n",
    "    pos_tags = QUERY_CLASS[NAME][POS]\n",
    "    \n",
    "    ans_candidate = get_ans_candidate(pos_candidate, pos_tags, POS)\n",
    "\n",
    "    return get_ans_string(sent_splt, ans_candidate)\n",
    "    \n",
    "    \n",
    "def get_binary_ans(my_sent, query):\n",
    "    sent_splt = nltk.word_tokenize(my_sent)\n",
    "    sent_splt_lemma = [lemmatize_word(x) for x in sent_splt]\n",
    "    print sent_splt\n",
    "    if 'true' in query:\n",
    "        for word in NOT_WORDS:\n",
    "            if word in sent_splt:\n",
    "                return 'false'\n",
    "        return 'true'\n",
    "    \n",
    "    pos_candidate = tag_sent(sent_splt, sent_splt_lemma, query, POS)\n",
    "    \n",
    "    ans_candidate = get_ans_candidate(pos_candidate, [], POS)\n",
    "\n",
    "    return get_ans_string(sent_splt, ans_candidate)\n",
    "\n",
    "\n",
    "def get_polar_ans(my_sent, query):\n",
    "    sent_splt = nltk.word_tokenize(my_sent)\n",
    "    sent_splt_lemma = [lemmatize_word(x) for x in sent_splt]\n",
    "    \n",
    "    for word in NOT_WORDS:\n",
    "        if word in sent_splt:\n",
    "            return 'no'\n",
    "        return 'yes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_how_ans(my_sent, query):\n",
    "    sent_splt = nltk.word_tokenize(my_sent)\n",
    "    sent_splt_lemma = [lemmatize_word(x) for x in sent_splt]\n",
    "        \n",
    "#     print sent_splt, sent_splt_lemma\n",
    "    \n",
    "    word_after_how = None\n",
    "    \n",
    "    for word in HOW_SUBCLASS.keys():\n",
    "        if word in query:\n",
    "            word_after_how = word\n",
    "            break\n",
    "    \n",
    "    if word_after_how:\n",
    "                \n",
    "        ner_candidate = tag_sent(sent_splt, sent_splt_lemma, query, NER)\n",
    "        ner_tags = HOW_SUBCLASS[word_after_how][NER]\n",
    "        ans_candidate = get_ans_candidate(ner_candidate, ner_tags, NER)\n",
    "        \n",
    "        \n",
    "        if ans_candidate == []:\n",
    "\n",
    "            pos_candidate = tag_sent(sent_splt, sent_splt_lemma, query, POS)\n",
    "#             print(\"POS_candidate\")\n",
    "#             print pos_candidate\n",
    "            \n",
    "            pos_tags = HOW_SUBCLASS[word_after_how][POS]\n",
    "#             print(\"POS TAGS\")\n",
    "#             print pos_tags\n",
    "            \n",
    "            ans_candidate = get_ans_candidate(pos_candidate, pos_tags, POS) \n",
    "            \n",
    "    else:\n",
    "        pos_candidate = tag_sent(sent_splt, sent_splt_lemma, query, POS)\n",
    "\n",
    "        if pos_candidate.has_key('IN'):\n",
    "            prep_index = pos_candidate['IN'].sort()[0]\n",
    "            sent_splt = sent_splt[prep_index+1:]\n",
    "            sent_splt_lemma = sent_splt_lemma[prep_index+1:]\n",
    "            pos_candidate = tag_sent(sent_splt, sent_splt_lemma, query, POS)\n",
    "        \n",
    "        ans_candidate = get_ans_candidate(pos_candidate, [], POS)\n",
    "    \n",
    "#     print ans_candidate\n",
    "    return get_ans_string(sent_splt, ans_candidate)\n",
    "\n",
    "def get_ans_candidate(candidate, tags, flag):\n",
    "    ans = []\n",
    "    \n",
    "    for tag in tags:\n",
    "#         print tag\n",
    "        if candidate.has_key(tag):\n",
    "#             print 'yes'\n",
    "            ans += candidate[tag]\n",
    "            # flag is NER (=0), break, get the words with one tag\n",
    "            # flag is POS (=1), not break, get words with all tags\n",
    "            if not flag:\n",
    "                break\n",
    "                \n",
    "    # flag is NER (=0), if no ans, return []\n",
    "    # flag is POS (=1), if no ans, return all words regradless of tags\n",
    "    if flag and ans == []:\n",
    "        for lst in candidate.values():\n",
    "            ans += lst\n",
    "        \n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def output_result(final_ans):\n",
    "    with open('names.csv', 'w') as csvfile:\n",
    "        fieldnames = ['id', 'answer']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for each_dict in final_ans:\n",
    "            writer.writerow({'id': each_dict['id'], 'answer': each_dict['answer']})\n",
    "    csvfile.close()\n",
    "\n",
    "def output_result_train(final_ans):\n",
    "    with open('names.csv', 'w') as csvfile:\n",
    "        fieldnames = ['ans', 'my_ans', 'prec', 'recall', 'f_score', 'Q', 'para', 'docid']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for each_dict in final_ans:\n",
    "            writer.writerow({'ans': each_dict['ans'], \n",
    "                             'my_ans': each_dict['my_ans'], \n",
    "                             'prec': each_dict['prec'], \n",
    "                             'recall': each_dict['recall'], \n",
    "                             'f_score': each_dict['f_score'], \n",
    "                             'Q': each_dict['Q'], \n",
    "                             'para': each_dict['para'], \n",
    "                             'docid': each_dict['docid']})\n",
    "    csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def tag_sent(sent_splt, sent_splt_lemma, query, tag_type):\n",
    "    \n",
    "    if tag_type == NER:\n",
    "        tagger = ner_tagger\n",
    "    else:\n",
    "        tagger = pos_tagger\n",
    "    \n",
    "    tagged = tagger.tag(sent_splt)\n",
    "    \n",
    "#     print tagged\n",
    "    \n",
    "    candidates = defaultdict(list)\n",
    "    \n",
    "    for i in range(len(sent_splt_lemma)):\n",
    "        word = sent_splt_lemma[i]\n",
    "#         print word\n",
    "        if word not in query and word not in stopwords and word not in PUNCTUATION:\n",
    "#                 print(\"yes\")\n",
    "                if tag_type == NER:\n",
    "                    candidates[tagged[i][1]].append(i)\n",
    "                else:\n",
    "                    candidates[tagged[i][1][:2]].append(i)\n",
    "    return candidates\n",
    "   \n",
    "def get_ans_string(sent_splt, ans_candidate):\n",
    "    words = [sent_splt[i].lower() for i in ans_candidate]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Below: get accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def get_para_accuracy(fname):\n",
    "#     counter = float(0)\n",
    "#     with open(fname) as json_data:\n",
    "#         infile = json.load(json_data)\n",
    "#         length = len(infile)\n",
    "#         for dic in infile:\n",
    "#             question = dic['question']\n",
    "#             ans = dic['text']\n",
    "#             para = dic['answer_paragraph']\n",
    "#             docid = dic['docid']    \n",
    "            \n",
    "#             query, q_type = extract_query(question)\n",
    "            \n",
    "# #             print question[:20], query\n",
    "            \n",
    "#             para_ord, result = get_para(docid, query)\n",
    "            \n",
    "#             if para_ord == para:\n",
    "#                 counter += 1\n",
    "\n",
    "#     return counter/length\n",
    "\n",
    "# develfname = 'project_files/mytest.json' # Need to change later\n",
    "# acc = get_para_accuracy(develfname)\n",
    "# print acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_sent(result_para, query):\n",
    "    sentences = sent_segmenter.tokenize(result_para)\n",
    "    raw_sents = list(parse_paragraphs(sentences))\n",
    "    sent_term_freqs, M = get_freq_dict(raw_sents)\n",
    "    sent_freqs = compute_para_freqs(sent_term_freqs)\n",
    "    results = query_vsm(query, get_index(sent_term_freqs, sent_freqs, M))\n",
    "    if results == []:\n",
    "        return raw_sents[0][1]\n",
    "    else:\n",
    "        return raw_sents[results[0][0]][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Q': u'On what date did the companies that became the Computing-Tabulating-Recording Company get consolidated?', 'docid': 380, 'f_score': 0.2916666666666667, 'para': 5, 'ans': u'june 16 , 1911', 'my_ans': u'june new york state charles ranlett flint ctr', 'prec': 0.20588235294117646, 'recall': 0.5}\n",
      "{'Q': u'What percentage of its desktop PCs does IBM plan to install Open Client on to?', 'docid': 380, 'f_score': 0.5, 'para': 22, 'ans': u'5 %', 'my_ans': u'5', 'prec': 1.0, 'recall': 0.3333333333333333}\n",
      "{'Q': u'What year did IBM hire its first black salesman?', 'docid': 380, 'f_score': 0, 'para': 16, 'ans': u'1946', 'my_ans': u'company civil rights act', 'prec': 0.0, 'recall': 0.0}\n",
      "{'Q': u'IBM made an acquisition in 2009, name it.', 'docid': 380, 'f_score': 0.12244897959183675, 'para': 4, 'ans': u'spss', 'my_ans': u'weather channel licensing agreement use data', 'prec': 0.06666666666666667, 'recall': 0.75}\n",
      "{'Q': u'This IBM invention is known by the acronym UPC, what is the full name?', 'docid': 380, 'f_score': 0.7499999999999999, 'para': 2, 'ans': u'universal product code', 'my_ans': u'research laboratories research', 'prec': 0.8333333333333334, 'recall': 0.6818181818181818}\n",
      "{'Q': u'In 2012 Fortune ranked the largest US firms by number employees, what was IBMs rank?', 'docid': 380, 'f_score': 0.5217391304347825, 'para': 1, 'ans': u'second largest', 'my_ans': u'ibm u.s. terms terms market capitalization nineteenth terms revenue', 'prec': 0.375, 'recall': 0.8571428571428571}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-c4fd1067da95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0mdevelfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'project_files/devel.json'\u001b[0m \u001b[0;31m# Need to change later\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_accuracy_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevelfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-21-c4fd1067da95>\u001b[0m in \u001b[0;36mget_accuracy_train\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;31m# my_sent is raw sent in the paragraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mmy_sent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_sent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_para\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mmy_ans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_sent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;31m# calculate precision, recall and accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-207b04951419>\u001b[0m in \u001b[0;36mget_ans\u001b[0;34m(my_sent, query, q_type)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_who_ans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_sent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mq_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mHOW\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_how_ans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_sent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mq_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mWHERE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_where_ans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_sent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-a729c1e7bc13>\u001b[0m in \u001b[0;36mget_how_ans\u001b[0;34m(my_sent, query)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mword_after_how\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mner_candidate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtag_sent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_splt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_splt_lemma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNER\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mner_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHOW_SUBCLASS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_after_how\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mNER\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mans_candidate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ans_candidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mner_candidate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mner_tags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNER\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-39c4595f6091>\u001b[0m in \u001b[0;36mtag_sent\u001b[0;34m(sent_splt, sent_splt_lemma, query, tag_type)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_tagger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtagged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_splt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#     print tagged\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yifan/Library/Enthought/Canopy/edm/envs/User/lib/python2.7/site-packages/nltk/tag/stanford.pyc\u001b[0m in \u001b[0;36mtag\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;31m# This function should return list of tuple rather than list of list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtag_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yifan/Library/Enthought/Canopy/edm/envs/User/lib/python2.7/site-packages/nltk/tag/stanford.pyc\u001b[0m in \u001b[0;36mtag_sents\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;31m# Run the tagger and get the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         stanpos_output, _stderr = java(cmd, classpath=self._stanford_jar,\n\u001b[0;32m---> 99\u001b[0;31m                                        stdout=PIPE, stderr=PIPE)\n\u001b[0m\u001b[1;32m    100\u001b[0m         \u001b[0mstanpos_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstanpos_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yifan/Library/Enthought/Canopy/edm/envs/User/lib/python2.7/site-packages/nltk/__init__.pyc\u001b[0m in \u001b[0;36mjava\u001b[0;34m(cmd, classpath, stdin, stdout, stderr, blocking)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstdin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mblocking\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;31m# Check the return code.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yifan/Library/Enthought/Canopy/edm/envs/User/lib/python2.7/subprocess.pyc\u001b[0m in \u001b[0;36mcommunicate\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 479\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_communicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yifan/Library/Enthought/Canopy/edm/envs/User/lib/python2.7/subprocess.pyc\u001b[0m in \u001b[0;36m_communicate\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m   1096\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_has_poll\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m                 \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_communicate_with_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_communicate_with_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yifan/Library/Enthought/Canopy/edm/envs/User/lib/python2.7/subprocess.pyc\u001b[0m in \u001b[0;36m_communicate_with_poll\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m   1150\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mfd2file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m                     \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpoller\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mselect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0merrno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEINTR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def get_accuracy_train(fname):\n",
    "    counter = float(0)\n",
    "    final_ans = []\n",
    "    with open(fname) as json_data:\n",
    "        infile = json.load(json_data)\n",
    "        length = len(infile)\n",
    "        for dic in infile:\n",
    "            \n",
    "            # training and devel\n",
    "            question = dic['question']\n",
    "            ans = dic['text']\n",
    "            para = dic['answer_paragraph']\n",
    "            docid = dic['docid']    \n",
    "            \n",
    "            query, q_type = extract_query(question)            \n",
    "            my_para_ord, my_para = get_para(docid, query)\n",
    "                        \n",
    "            # my_sent is raw sent in the paragraph\n",
    "            my_sent = get_sent(my_para, query)\n",
    "            my_ans = get_ans(my_sent, query, q_type)\n",
    "            \n",
    "            # calculate precision, recall and accuracy \n",
    "            tp = 0.0\n",
    "            fp = 0.0\n",
    "            fn = 0.0\n",
    "            \n",
    "            for word in ans:\n",
    "                if word in my_ans:\n",
    "                    tp += 1\n",
    "                else:\n",
    "                    fn += 1\n",
    "            for word in my_ans:\n",
    "                if word not in ans:\n",
    "                    fp += 1\n",
    "            \n",
    "            # prec = TP / (TP + FP) \n",
    "            prec = tp / (tp+fp)\n",
    "            # recall = TP / (TP + FN) \n",
    "            recall = tp / (tp+fn)\n",
    "            if prec+recall == 0:\n",
    "                f_score = 0\n",
    "            else:\n",
    "                f_score = 2*prec*recall / (prec+recall)\n",
    "            # end\n",
    "            \n",
    "            rst_dict = {'ans': ans, \n",
    "                        'my_ans': my_ans, \n",
    "                        'prec': prec, \n",
    "                        'recall': recall, \n",
    "                        'f_score': f_score,\n",
    "                        'Q': question, \n",
    "                        'para': para, \n",
    "                        'docid': docid}\n",
    "            \n",
    "            print rst_dict\n",
    "            \n",
    "            final_ans.append(rst_dict)\n",
    "    output_result_train(final_ans)\n",
    "    \n",
    "\n",
    "develfname = 'project_files/devel.json' # Need to change later\n",
    "acc = get_accuracy_train(develfname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_final_result(fname):\n",
    "    counter = float(0)\n",
    "    final_ans = []\n",
    "    with open(fname) as json_data:\n",
    "        infile = json.load(json_data)\n",
    "        length = len(infile)\n",
    "        for dic in infile:  \n",
    "            \n",
    "            #testing\n",
    "            question = dic['question']\n",
    "            docid = dic['docid'] \n",
    "            # question id\n",
    "            qid = dic['id']\n",
    "            \n",
    "            query, q_type = extract_query(question)\n",
    "                        \n",
    "            my_para_ord, my_para = get_para(docid, query)\n",
    "                        \n",
    "            # my_sent is raw sent in the paragraph\n",
    "            my_sent = get_sent(my_para, query)\n",
    "            \n",
    "            my_ans = get_ans(my_sent, query, q_type)\n",
    "            \n",
    "            final_ans.append({'id': qid, 'answer': my_ans})\n",
    "            \n",
    "    output_result(final_ans)\n",
    "    \n",
    "develfname = 'project_files/testing.json' # Need to change later\n",
    "get_final_result(develfname)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
