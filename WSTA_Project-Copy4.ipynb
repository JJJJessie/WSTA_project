{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Team Name:     XiaoJiLingGui\n",
    "\n",
    "Kaggle Login:  siqiguo\n",
    "\n",
    "Student Name:  Siqi Guo\n",
    "\n",
    "Student ID:    743053\n",
    "\n",
    "Kaggle Login: yifan66\n",
    "\n",
    "Student Name: Yifan Wang\n",
    "\n",
    "Student ID: 784386\n",
    "\n",
    "Python version used: 2.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import json, string\n",
    "from math import log\n",
    "from collections import defaultdict, Counter\n",
    "import csv\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tag.stanford import StanfordPOSTagger\n",
    "\n",
    "stopwords = set(nltk.corpus.stopwords.words('english')) # wrap in a set() (see below)\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer() \n",
    "sent_segmenter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "DOCFNAME = 'project_files/documents.json'\n",
    "\n",
    "WHAT = 'what'\n",
    "WHO = 'who'\n",
    "HOW = 'how'\n",
    "WHERE = 'where'\n",
    "WHEN = 'when'\n",
    "WHICH = 'which'\n",
    "NAME = 'name'\n",
    "WHY = 'why'\n",
    "WHOM = 'whom'\n",
    "BINARY = 'binary'\n",
    "POLAR = 'polar'\n",
    "\n",
    "PUNCTUATION = string.punctuation.translate(None, '$%')\n",
    "\n",
    "WHAT_CD_WORDS = ['year', 'date', 'percentage', 'value', 'margin']\n",
    "\n",
    "NER = 0\n",
    "\n",
    "POS = 1\n",
    "\n",
    "NOT_WORDS = ['not', \"n't\"]\n",
    "\n",
    "REASON_WORDS = ['becauce', 'since', 'due to', 'thanks to', 'owing to', \n",
    "                'on account of', 'result', 'for']\n",
    "\n",
    "HOW_SUBCLASS = {'many': [[], ['CD', 'FW']], \n",
    "                'long': [['DURATION'], ['CD', 'FW']], \n",
    "                'much': [['MONEY'], ['$', 'CD', 'FW', 'NN']], \n",
    "                'far': [[], ['CD', 'FW']], \n",
    "                'tall': [[], ['CD', 'FW']], \n",
    "                'rich': [['MONEY'], ['$', 'CD', 'FW']], \n",
    "                'large': [[], ['CD', 'FW']], \n",
    "                }\n",
    "\n",
    "QUERY_CLASS = {WHAT: [[], ['NN', 'FW']],\n",
    "               # To optimise, not include WHO - 'ORGANIZATION' in the tag\n",
    "               WHO: [['PERSON'], ['NN', 'FW']], \n",
    "               WHERE: [['LOCATION'], ['NN', 'FW']],\n",
    "               WHEN: [['DATE', 'TIME'], ['CD', 'NN', 'FW']],\n",
    "               WHICH: [['PERSON', 'LOCATION', 'DATE', 'TIME', 'ORGANIZATION'], ['NN', 'FW']],\n",
    "               NAME: [['PERSON', 'LOCATION', 'ORGANIZATION'], ['NN', 'FW']],\n",
    "               #'why': reason, find \"reason word\" \n",
    "               WHOM: [['PERSON'], ['NN', 'FW']],\n",
    "               \n",
    "               # LOCATION, sometimes 'CD' also included as street number, unit number \n",
    "               # what: if and not in query terms, then \n",
    "              \n",
    "              }\n",
    "\n",
    "inverted_index_dict = {} # Store the processed documents' inverted index to improve the effiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ner_dir = 'stanford-ner-2018-02-27/'\n",
    "ner_jarfile = ner_dir + 'stanford-ner.jar'\n",
    "ner_modelfile = ner_dir + 'classifiers/english.all.3class.distsim.crf.ser.gz'\n",
    "ner_tagger = StanfordNERTagger(model_filename=ner_modelfile, path_to_jar=ner_jarfile)\n",
    "\n",
    "pos_dir = 'stanford-postagger-2018-02-27/'\n",
    "pos_modelfile = pos_dir + 'models/english-bidirectional-distsim.tagger'\n",
    "pos_jarfile = pos_dir + 'stanford-postagger.jar'\n",
    "pos_tagger = StanfordPOSTagger(model_filename=pos_modelfile, path_to_jar=pos_jarfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def lemmatize_word(word):\n",
    "    word = word.lower()\n",
    "#     print word\n",
    "    adj_word = get_adj(word)\n",
    "    if adj_word != None:\n",
    "        return adj_word\n",
    "    lemma_n = lemmatizer.lemmatize(word, 'n')\n",
    "    lemma_v = lemmatizer.lemmatize(word, 'v')\n",
    "    \n",
    "    # If both change, return the shorter one\n",
    "    # If only one change, return the changed one\n",
    "    # If neither change, return the original word\n",
    "    \n",
    "    if lemma_n != word and lemma_v != word:    \n",
    "        if len(lemma_n) < len(lemma_v):\n",
    "            return lemma_n\n",
    "        else:\n",
    "            return lemma_v\n",
    "    elif lemma_n == word and lemma_v == word:\n",
    "        return word\n",
    "    elif lemma_n != word:\n",
    "        return lemma_n\n",
    "    elif lemma_v != word:\n",
    "        return lemma_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_adj(word):\n",
    "    for syn in wn.synsets(word):\n",
    "        syn_split = syn.name().split('.')\n",
    "        if syn_split[0] == word and syn_split[1] == 'r':\n",
    "            for lemmas in syn.lemmas(): # all possible lemmas\n",
    "                if lemmas.name() == word:\n",
    "                    if lemmas.pertainyms() == []:\n",
    "                        return None\n",
    "                    return lemmas.pertainyms()[0].name()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# lemmatize_word('lakes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Select the most relevant paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def extract_query(query):\n",
    "    tokenized = nltk.word_tokenize(query)\n",
    "    tokenized = [x.lower() for x in tokenized]\n",
    "    q_type = None\n",
    "    \n",
    "    if WHAT in tokenized:\n",
    "        q_type = WHAT\n",
    "                \n",
    "    elif WHO in tokenized:\n",
    "        q_type = WHO\n",
    "        \n",
    "    elif HOW in tokenized: \n",
    "        q_type = HOW\n",
    "        # Since HOW queries have many answer types, furthrer analysis required\n",
    "#         how_index = tokenized.index(HOW)\n",
    "            \n",
    "#         how_sub = tokenized[how_index + 1]\n",
    "        \n",
    "#         if how_sub in HOW_SUBCLASS.keys():\n",
    "#             tokenized.pop(how_index + 1)\n",
    "#             q_type = how_sub\n",
    "#         else:\n",
    "\n",
    "    elif WHERE in tokenized:\n",
    "        q_type = WHERE\n",
    "     \n",
    "    elif WHEN in tokenized:\n",
    "        # Use SUTime\n",
    "        q_type = WHEN\n",
    "    \n",
    "    elif WHICH in tokenized:\n",
    "        q_type = WHICH\n",
    "        \n",
    "    elif WHY in tokenized:\n",
    "        q_type = WHY\n",
    "    \n",
    "    elif WHOM in tokenized:\n",
    "        q_type = WHOM\n",
    "    \n",
    "    elif NAME in tokenized:\n",
    "        q_type = NAME\n",
    "    else:\n",
    "        if 'or' in tokenized:\n",
    "            q_type = BINARY\n",
    "        else:\n",
    "            q_type = POLAR\n",
    "        \n",
    "    terms = []\n",
    "    for token in tokenized:\n",
    "        if token not in stopwords and token not in PUNCTUATION: \n",
    "            terms.append(lemmatize_word(token))\n",
    "            \n",
    "    return terms, q_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def select_document(fname, docid):\n",
    "    with open(fname) as json_data:\n",
    "        infile = json.load(json_data)\n",
    "        for doc in infile:\n",
    "            if doc['docid'] == docid:\n",
    "                text = doc['text']\n",
    "    return text\n",
    "        \n",
    "\n",
    "def parse_paragraphs(text):\n",
    "    identifier = 0\n",
    "    for para in text:\n",
    "        yield (identifier, para)\n",
    "        identifier += 1\n",
    "\n",
    "def extract_term_freqs(para):\n",
    "    tfs = Counter()\n",
    "    for token in nltk.word_tokenize(para):\n",
    "        if token not in stopwords and token not in PUNCTUATION: # 'in' and 'not in' operations are much faster over sets that lists\n",
    "            tfs[lemmatize_word(token)] += 1\n",
    "    return tfs\n",
    "\n",
    "def compute_para_freqs(para_term_freqs):\n",
    "    pfs = Counter()\n",
    "    for tfs in para_term_freqs.values():\n",
    "        for term in tfs.keys():\n",
    "            pfs[term] += 1\n",
    "    return pfs\n",
    "\n",
    "def get_index(para_term_freqs, para_freqs, M):\n",
    "    vsm_inverted_index = defaultdict(list)\n",
    "    for pid, term_freqs in para_term_freqs.items():\n",
    "        N = sum(term_freqs.values())\n",
    "        length = 0\n",
    "\n",
    "        # find tf*idf values and accumulate sum of squares \n",
    "        tfidf_values = []\n",
    "        for term, count in term_freqs.items():\n",
    "            tfidf = float(count) / N * log(M / float(para_freqs[term]))\n",
    "            tfidf_values.append((term, tfidf))\n",
    "            length += tfidf ** 2\n",
    "\n",
    "        # normalise documents by length and insert into index\n",
    "        length = length ** 0.5\n",
    "        for term, tfidf in tfidf_values:\n",
    "            # note the inversion of the indexing, to be term -> (doc_id, score)\n",
    "            vsm_inverted_index[term].append([pid, tfidf / length])\n",
    "\n",
    "    # ensure posting lists are in sorted order (less important here cf above)\n",
    "    for term, pids in vsm_inverted_index.items():\n",
    "        pids.sort()\n",
    "    return vsm_inverted_index\n",
    "    \n",
    "def query_vsm(query, index, k=4): # Only return the most revelant paragraph\n",
    "    accumulator = Counter()\n",
    "    for term in query:\n",
    "        postings = index[term]\n",
    "        for pid, weight in postings:\n",
    "            accumulator[pid] += weight\n",
    "    return accumulator.most_common(k)\n",
    "\n",
    "def get_freq_dict(raw_paras):\n",
    "    para_term_freqs = {}\n",
    "    for pid, para in raw_paras:\n",
    "        term_freqs = extract_term_freqs(para)\n",
    "        para_term_freqs[pid] = term_freqs\n",
    "    M = len(para_term_freqs)\n",
    "    return para_term_freqs, M\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_para(docid, query):\n",
    "    text = select_document(DOCFNAME, docid)\n",
    "    raw_paras = list(parse_paragraphs(text))\n",
    "    if len(raw_paras) == 1: # If there's only one paragraph in the document\n",
    "        return 0, raw_paras[0][1]\n",
    "    if docid not in inverted_index_dict.keys(): # check whether the document has been processed before\n",
    "        para_term_freqs, M = get_freq_dict(raw_paras)\n",
    "        \n",
    "        para_freqs = compute_para_freqs(para_term_freqs)\n",
    "        inverted_index = get_index(para_term_freqs, para_freqs, M)\n",
    "        inverted_index_dict[docid] = inverted_index\n",
    "    else:\n",
    "        inverted_index = inverted_index_dict[docid]\n",
    "    results = query_vsm(query, inverted_index)\n",
    "    if results == []:\n",
    "        return 0, raw_paras[0][1]\n",
    "    else:\n",
    "        return results[0][0], raw_paras[results[0][0]][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_sent(result_para, query):\n",
    "    sentences = sent_segmenter.tokenize(result_para)\n",
    "    raw_sents = list(parse_paragraphs(sentences))\n",
    "    if len(raw_sents) == 1: # If there's only one sentence in the paragraph\n",
    "        return raw_sents[0][1]\n",
    "    sent_term_freqs, M = get_freq_dict(raw_sents)\n",
    "    sent_freqs = compute_para_freqs(sent_term_freqs)\n",
    "    results = query_vsm(query, get_index(sent_term_freqs, sent_freqs, M))\n",
    "    if results == []:\n",
    "        return raw_sents[0][1]\n",
    "    else:\n",
    "        return raw_sents[results[0][0]][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_ans(my_sent, query, q_type):\n",
    "    \n",
    "    ans = ''\n",
    "    \n",
    "    if q_type == WHAT:\n",
    "        ans = get_what_ans(my_sent, query)\n",
    "    elif q_type == WHO:\n",
    "        ans = get_who_ans(my_sent, query)\n",
    "    elif q_type == HOW:  \n",
    "        ans = get_how_ans(my_sent, query)\n",
    "    elif q_type == WHERE:\n",
    "        ans = get_where_ans(my_sent, query)\n",
    "    elif q_type == WHEN:\n",
    "        ans = get_when_ans(my_sent, query)\n",
    "    elif q_type == WHICH:\n",
    "        ans = get_which_ans(my_sent, query)\n",
    "    elif q_type == WHY:\n",
    "        ans = get_why_ans(my_sent, query)\n",
    "    elif q_type == WHOM:\n",
    "        ans = get_whom_ans(my_sent, query)\n",
    "    elif q_type == NAME:\n",
    "        ans = get_name_ans(my_sent, query)\n",
    "    elif q_type == BINARY:\n",
    "        ans = get_binary_ans(my_sent, query)\n",
    "    elif q_type == POLAR:\n",
    "        ans = get_polar_ans(my_sent, query)  \n",
    "        \n",
    "    if ans == '':\n",
    "        return my_sent\n",
    "    else:\n",
    "        return ans\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_what_ans(my_sent, query):\n",
    "    sent_splt = nltk.word_tokenize(my_sent)\n",
    "    sent_splt_lemma = [lemmatize_word(x) for x in sent_splt]\n",
    "    \n",
    "    pos_candidate = tag_sent(sent_splt, sent_splt_lemma, query, POS)\n",
    "    \n",
    "    pos_tags = QUERY_CLASS[WHAT][POS][:]\n",
    "#     print pos_tags\n",
    "    \n",
    "    # special case: \"what color\"\n",
    "    if 'color' in query:\n",
    "        pos_tags.append('JJ')\n",
    "\n",
    "    # special case: what for an answer include numbers\n",
    "    for word in query:\n",
    "#         print pos_tags\n",
    "        if word in WHAT_CD_WORDS:\n",
    "            pos_tags.append('CD')\n",
    "            pos_tags.remove('NN')\n",
    "            break\n",
    "    \n",
    "    ans_candidate = get_ans_candidate(pos_candidate, pos_tags, POS)\n",
    "\n",
    "    if 'percentage' in query and '%' in sent_splt_lemma:\n",
    "        ans_candidate.append(sent_splt_lemma.index('%'))\n",
    "    \n",
    "    return get_ans_string(sent_splt, ans_candidate)\n",
    "    \n",
    "def get_who_ans(my_sent, query):\n",
    "    sent_splt = nltk.word_tokenize(my_sent)\n",
    "    sent_splt_lemma = [lemmatize_word(x) for x in sent_splt]\n",
    "    \n",
    "    ner_candidate = tag_sent(sent_splt, sent_splt_lemma, query, NER)\n",
    "    \n",
    "    ner_tags = QUERY_CLASS[WHO][NER][:]\n",
    "    \n",
    "    ans_candidate = get_ans_candidate(ner_candidate, ner_tags, NER)\n",
    "    \n",
    "    if ans_candidate == []:\n",
    "        pos_candidate = tag_sent(sent_splt, sent_splt_lemma, query, POS)\n",
    "        pos_tags = QUERY_CLASS[WHO][POS][:]\n",
    "        ans_candidate = get_ans_candidate(pos_candidate, pos_tags, POS)\n",
    "        \n",
    "    return get_ans_string(sent_splt, ans_candidate)\n",
    "    \n",
    "def get_where_ans(my_sent, query):\n",
    "    sent_splt = nltk.word_tokenize(my_sent)\n",
    "    sent_splt_lemma = [lemmatize_word(x) for x in sent_splt]\n",
    "    \n",
    "    ner_candidate = tag_sent(sent_splt, sent_splt_lemma, query, NER)\n",
    "    \n",
    "    ner_tags = QUERY_CLASS[WHERE][NER][:]\n",
    "    \n",
    "    ans_candidate = get_ans_candidate(ner_candidate, ner_tags, NER)\n",
    "    \n",
    "    if ans_candidate == []:\n",
    "        pos_candidate = tag_sent(sent_splt, sent_splt_lemma, query, POS)\n",
    "        pos_tags = QUERY_CLASS[WHERE][POS][:]\n",
    "        ans_candidate = get_ans_candidate(pos_candidate, pos_tags, POS)\n",
    "        \n",
    "    return get_ans_string(sent_splt, ans_candidate)\n",
    "    \n",
    "def get_when_ans(my_sent, query):\n",
    "    sent_splt = nltk.word_tokenize(my_sent)\n",
    "    sent_splt_lemma = [lemmatize_word(x) for x in sent_splt]\n",
    "    \n",
    "    ner_candidate = tag_sent(sent_splt, sent_splt_lemma, query, NER)\n",
    "    \n",
    "    ner_tags = QUERY_CLASS[WHEN][NER][:]\n",
    "    \n",
    "    ans_candidate = get_ans_candidate(ner_candidate, ner_tags, NER)\n",
    "    \n",
    "    if ans_candidate == []:\n",
    "        pos_candidate = tag_sent(sent_splt, sent_splt_lemma, query, POS)\n",
    "        pos_tags = QUERY_CLASS[WHEN][POS][:]\n",
    "        ans_candidate = get_ans_candidate(pos_candidate, pos_tags, POS)\n",
    "        \n",
    "    return get_ans_string(sent_splt, ans_candidate)\n",
    "    \n",
    "def get_which_ans(my_sent, query):\n",
    "    sent_splt = nltk.word_tokenize(my_sent)\n",
    "    sent_splt_lemma = [lemmatize_word(x) for x in sent_splt]\n",
    "    \n",
    "    ner_candidate = tag_sent(sent_splt, sent_splt_lemma, query, NER)\n",
    "    \n",
    "    ner_tags = QUERY_CLASS[WHICH][NER][:]\n",
    "    \n",
    "    ans_candidate = get_ans_candidate(ner_candidate, ner_tags, NER)\n",
    "    \n",
    "    if ans_candidate == []:\n",
    "        pos_candidate = tag_sent(sent_splt, sent_splt_lemma, query, POS)\n",
    "        pos_tags = QUERY_CLASS[WHICH][POS][:]\n",
    "        ans_candidate = get_ans_candidate(pos_candidate, pos_tags, POS)\n",
    "        \n",
    "    return get_ans_string(sent_splt, ans_candidate)\n",
    "\n",
    "    \n",
    "def get_why_ans(my_sent, query):\n",
    "    \n",
    "    for word in REASON_WORDS:\n",
    "        if word in my_sent:\n",
    "            my_sent = my_sent.split(word,1)[1] \n",
    "            \n",
    "    sent_splt = nltk.word_tokenize(my_sent)\n",
    "    \n",
    "    # special case - \"on one's account\" / \"on this account\"\n",
    "    if ('account' in sent_splt and 'on' in sent_splt \n",
    "        and (sent_splt.index('account') == sent_splt.index('on')+2 or \n",
    "             sent_splt.index('account') == sent_splt.index('on')+3)):\n",
    "        \n",
    "        sent_splt.remove('account')\n",
    "        sent_splt.remove('on')\n",
    "        \n",
    "    \n",
    "    sent_splt_lemma = [lemmatize_word(x) for x in sent_splt]\n",
    "    \n",
    "    ans_candidate = []\n",
    "    for i in range(len(sent_splt_lemma)):\n",
    "        word = sent_splt_lemma[i]\n",
    "        if (word not in query and word not in stopwords and \n",
    "            word not in PUNCTUATION):\n",
    "            ans_candidate.append(i)\n",
    "\n",
    "    return get_ans_string(sent_splt, ans_candidate)\n",
    "    \n",
    "def get_whom_ans(my_sent, query):\n",
    "    sent_splt = nltk.word_tokenize(my_sent)\n",
    "    sent_splt_lemma = [lemmatize_word(x) for x in sent_splt]\n",
    "    pos_candidate = tag_sent(sent_splt, sent_splt_lemma, query, POS)\n",
    "\n",
    "    if pos_candidate.has_key('IN'):\n",
    "        prep_index = pos_candidate['IN'].sort()[0]\n",
    "        sent_splt = sent_splt[prep_index+1:]\n",
    "        sent_splt_lemma = sent_splt_lemma[prep_index+1:]\n",
    "        pos_candidate = tag_sent(sent_splt, sent_splt_lemma, query, POS)\n",
    "\n",
    "    ner_candidate = tag_sent(sent_splt, sent_splt_lemma, query, NER)\n",
    "    \n",
    "    ner_tags = QUERY_CLASS[WHOM][NER][:]\n",
    "    \n",
    "    ans_candidate = get_ans_candidate(ner_candidate, ner_tags, NER)\n",
    "    \n",
    "    if ans_candidate == []:\n",
    "        pos_tags = QUERY_CLASS[WHOM][POS][:]\n",
    "        ans_candidate = get_ans_candidate(pos_candidate, pos_tags, POS)\n",
    "        \n",
    "    return get_ans_string(sent_splt, ans_candidate)\n",
    "    \n",
    "def get_name_ans(my_sent, query):\n",
    "    sent_splt = nltk.word_tokenize(my_sent)\n",
    "    sent_splt_lemma = [lemmatize_word(x) for x in sent_splt]\n",
    "    \n",
    "    pos_candidate = tag_sent(sent_splt, sent_splt_lemma, query, POS)\n",
    "    \n",
    "    pos_tags = QUERY_CLASS[NAME][POS][:]\n",
    "    \n",
    "    ans_candidate = get_ans_candidate(pos_candidate, pos_tags, POS)\n",
    "\n",
    "    return get_ans_string(sent_splt, ans_candidate)\n",
    "    \n",
    "    \n",
    "def get_binary_ans(my_sent, query):\n",
    "    sent_splt = nltk.word_tokenize(my_sent)\n",
    "    sent_splt_lemma = [lemmatize_word(x) for x in sent_splt]\n",
    "#     print sent_splt\n",
    "    if 'true' in query:\n",
    "        for word in NOT_WORDS:\n",
    "            if word in sent_splt:\n",
    "                return 'false'\n",
    "        return 'true'\n",
    "    \n",
    "    pos_candidate = tag_sent(sent_splt, sent_splt_lemma, query, POS)\n",
    "    \n",
    "    ans_candidate = get_ans_candidate(pos_candidate, [], POS)\n",
    "\n",
    "    return get_ans_string(sent_splt, ans_candidate)\n",
    "\n",
    "\n",
    "def get_polar_ans(my_sent, query):\n",
    "    sent_splt = nltk.word_tokenize(my_sent)\n",
    "    sent_splt_lemma = [lemmatize_word(x) for x in sent_splt]\n",
    "    \n",
    "    for word in NOT_WORDS:\n",
    "        if word in sent_splt:\n",
    "            return 'no'\n",
    "        return 'yes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_ans_candidate(candidate, tags, flag):\n",
    "    ans = []\n",
    "    \n",
    "    for tag in tags:\n",
    "#         print tag\n",
    "        if candidate.has_key(tag):\n",
    "#             print 'yes'\n",
    "            ans += candidate[tag]\n",
    "            # flag is NER (=0), break, get the words with one tag\n",
    "            # flag is POS (=1), not break, get words with all tags\n",
    "            if not flag:\n",
    "                break\n",
    "                \n",
    "    # flag is NER (=0), if no ans, return []\n",
    "    # flag is POS (=1), if no ans, return all words regradless of tags\n",
    "    if flag and ans == []:\n",
    "        for lst in candidate.values():\n",
    "            ans += lst\n",
    "        \n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def output_result(final_ans):\n",
    "    with open('names.csv', 'w') as csvfile:\n",
    "        fieldnames = ['id', 'answer']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for each_dict in final_ans:\n",
    "            writer.writerow({'id': each_dict['id'], 'answer': each_dict['answer'].encode(\"utf-8\")})\n",
    "    csvfile.close()\n",
    "    \n",
    "def output_one_result(final_ans):\n",
    "    with open('names.csv', 'a') as csvfile:\n",
    "        fieldnames = ['id', 'answer']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writerow(final_ans)\n",
    "    csvfile.close()\n",
    "\n",
    "def output_result_train(final_ans):\n",
    "    with open('names.csv', 'w') as csvfile:\n",
    "        fieldnames = ['f_score', 'ans', 'my_ans', 'prec', 'recall', 'Q', 'para', 'docid']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for each_dict in final_ans:\n",
    "            writer.writerow({'f_score': each_dict['f_score'],\n",
    "                             'ans': each_dict['ans'].encode(\"utf-8\"), \n",
    "                             'my_ans': each_dict['my_ans'].encode(\"utf-8\"), \n",
    "                             'prec': each_dict['prec'], \n",
    "                             'recall': each_dict['recall'], \n",
    "                             'Q': each_dict['Q'].encode(\"utf-8\"), \n",
    "                             'para': each_dict['para'], \n",
    "                             'docid': each_dict['docid']})\n",
    "    csvfile.close()\n",
    "    \n",
    "def output_one_result_train(final_ans):\n",
    "    with open('names.csv', 'a') as csvfile:\n",
    "        fieldnames = ['f_score', 'ans', 'my_ans', 'prec', 'recall', 'Q', 'para', 'docid']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writerow({'f_score': final_ans['f_score'],\n",
    "                         'ans': final_ans['ans'].encode(\"utf-8\"), \n",
    "                         'my_ans': final_ans['my_ans'].encode(\"utf-8\"), \n",
    "                         'prec': final_ans['prec'], \n",
    "                         'recall': final_ans['recall'], \n",
    "                         'Q': final_ans['Q'].encode(\"utf-8\"), \n",
    "                         'para': final_ans['para'], \n",
    "                         'docid': final_ans['docid']})\n",
    "    csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def tag_sent(sent_splt, sent_splt_lemma, query, tag_type):\n",
    "    \n",
    "    if tag_type == NER:\n",
    "        tagger = ner_tagger\n",
    "    else:\n",
    "        tagger = pos_tagger\n",
    "    \n",
    "    tagged = tagger.tag(sent_splt)\n",
    "    \n",
    "#     print tagged\n",
    "    \n",
    "    candidates = defaultdict(list)\n",
    "    \n",
    "    for i in range(len(sent_splt_lemma)):\n",
    "        word = sent_splt_lemma[i]\n",
    "#         print word\n",
    "        if word not in query and word not in stopwords and word not in PUNCTUATION:\n",
    "#                 print(\"yes\")\n",
    "                if tag_type == NER:\n",
    "                    candidates[tagged[i][1]].append(i)\n",
    "                else:\n",
    "                    candidates[tagged[i][1][:2]].append(i)\n",
    "    return candidates\n",
    "   \n",
    "def get_ans_string(sent_splt, ans_candidate):\n",
    "    words = set([sent_splt[i].lower() for i in ans_candidate])\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Below: get accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def get_para_accuracy(fname):\n",
    "#     counter = float(0)\n",
    "#     with open(fname) as json_data:\n",
    "#         infile = json.load(json_data)\n",
    "#         length = len(infile)\n",
    "#         for dic in infile:\n",
    "#             question = dic['question']\n",
    "#             ans = dic['text']\n",
    "#             para = dic['answer_paragraph']\n",
    "#             docid = dic['docid']    \n",
    "            \n",
    "#             query, q_type = extract_query(question)\n",
    "            \n",
    "# #             print question[:20], query\n",
    "            \n",
    "#             para_ord, result = get_para(docid, query)\n",
    "            \n",
    "#             if para_ord == para:\n",
    "#                 counter += 1\n",
    "\n",
    "#     return counter/length\n",
    "\n",
    "# develfname = 'project_files/mytest.json' # Need to change later\n",
    "# acc = get_para_accuracy(develfname)\n",
    "# print acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def get_accuracy_train(fname):\n",
    "#     counter = float(0)\n",
    "    \n",
    "#     with open('names.csv', 'w') as csvfile:\n",
    "#         fieldnames = ['f_score', 'ans', 'my_ans', 'prec', 'recall', 'Q', 'para', 'docid']\n",
    "#         writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "#         writer.writeheader()\n",
    "#     csvfile.close()\n",
    "    \n",
    "#     with open(fname) as json_data:\n",
    "#         infile = json.load(json_data)\n",
    "#         length = len(infile)\n",
    "#         for dic in infile:\n",
    "            \n",
    "#             # training and devel\n",
    "#             question = dic['question']\n",
    "#             ans = dic['text']\n",
    "#             para = dic['answer_paragraph']\n",
    "#             docid = dic['docid']    \n",
    "            \n",
    "#             query, q_type = extract_query(question)            \n",
    "#             my_para_ord, my_para = get_para(docid, query)\n",
    "                        \n",
    "#             # my_sent is raw sent in the paragraph\n",
    "#             my_sent = get_sent(my_para, query)\n",
    "#             my_ans = get_ans(my_sent, query, q_type)\n",
    "            \n",
    "#             # calculate precision, recall and accuracy \n",
    "#             tp = 0.0\n",
    "#             fp = 0.0\n",
    "#             fn = 0.0\n",
    "            \n",
    "#             for word in ans:\n",
    "#                 if word in my_ans:\n",
    "#                     tp += 1\n",
    "#                 else:\n",
    "#                     fn += 1\n",
    "#             for word in my_ans:\n",
    "#                 if word not in ans:\n",
    "#                     fp += 1\n",
    "            \n",
    "#             # prec = TP / (TP + FP) \n",
    "#             prec = tp / (tp+fp)\n",
    "#             # recall = TP / (TP + FN) \n",
    "#             recall = tp / (tp+fn)\n",
    "#             if prec+recall == 0:\n",
    "#                 f_score = 0\n",
    "#             else:\n",
    "#                 f_score = 2*prec*recall / (prec+recall)\n",
    "#             # end\n",
    "            \n",
    "#             rst_dict = {'f_score': f_score,\n",
    "#                         'ans': ans, \n",
    "#                         'my_ans': my_ans, \n",
    "#                         'prec': prec, \n",
    "#                         'recall': recall, \n",
    "#                         'Q': question, \n",
    "#                         'para': para, \n",
    "#                         'docid': docid}\n",
    "            \n",
    "#             output_one_result_train(rst_dict)\n",
    "            \n",
    "#             print f_score\n",
    "            \n",
    "            \n",
    "            \n",
    "# develfname = 'project_files/devel_some.json' # Need to change later\n",
    "# acc = get_accuracy_train(develfname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# def get_final_result(fname):\n",
    "#     counter = float(0)\n",
    "#     final_ans = []\n",
    "#     with open(fname) as json_data:\n",
    "#         infile = json.load(json_data)\n",
    "#         length = len(infile)\n",
    "#         for dic in infile:  \n",
    "            \n",
    "#             #testing\n",
    "#             question = dic['question']\n",
    "#             docid = dic['docid'] \n",
    "#             # question id\n",
    "#             qid = dic['id']\n",
    "            \n",
    "#             query, q_type = extract_query(question)\n",
    "                        \n",
    "#             my_para_ord, my_para = get_para(docid, query)\n",
    "                        \n",
    "#             # my_sent is raw sent in the paragraph\n",
    "#             my_sent = get_sent(my_para, query)\n",
    "            \n",
    "#             my_ans = get_ans(my_sent, query, q_type)\n",
    "            \n",
    "#             print qid\n",
    "            \n",
    "#             final_ans.append({'id': qid, 'answer': my_ans})\n",
    "            \n",
    "#     output_result(final_ans)\n",
    "    \n",
    "# develfname = 'project_files/testing.json' # Need to change later\n",
    "# get_final_result(develfname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_how_ans(my_sent, query):\n",
    "    sent_splt = nltk.word_tokenize(my_sent)\n",
    "    sent_splt_lemma = [lemmatize_word(x) for x in sent_splt]\n",
    "        \n",
    "#     print sent_splt, sent_splt_lemma\n",
    "    \n",
    "    word_after_how = None\n",
    "    \n",
    "    for word in HOW_SUBCLASS.keys():\n",
    "        if word in query:\n",
    "            word_after_how = word\n",
    "            break\n",
    "    \n",
    "    if word_after_how:\n",
    "                \n",
    "        ner_candidate = tag_sent(sent_splt, sent_splt_lemma, query, NER)\n",
    "        ner_tags = HOW_SUBCLASS[word_after_how][NER]\n",
    "        ans_candidate = get_ans_candidate(ner_candidate, ner_tags, NER)\n",
    "        \n",
    "        \n",
    "        if ans_candidate == []:\n",
    "\n",
    "            pos_candidate = tag_sent(sent_splt, sent_splt_lemma, query, POS)\n",
    "#             print(\"POS_candidate\")\n",
    "#             print pos_candidate\n",
    "            \n",
    "            pos_tags = HOW_SUBCLASS[word_after_how][POS]\n",
    "#             print(\"POS TAGS\")\n",
    "#             print pos_tags\n",
    "            \n",
    "            ans_candidate = get_ans_candidate(pos_candidate, pos_tags, POS) \n",
    "            \n",
    "    else:\n",
    "        pos_candidate = tag_sent(sent_splt, sent_splt_lemma, query, POS)\n",
    "        \n",
    "        print pos_candidate\n",
    "\n",
    "        if pos_candidate.has_key('IN'):\n",
    "            print pos_candidate['IN']\n",
    "            print pos_candidate['IN'].sort()\n",
    "            \n",
    "            pos_candidate['IN'].sort()\n",
    "            prep_index = pos_candidate['IN'][0]\n",
    "            sent_splt = sent_splt[prep_index+1:]\n",
    "            sent_splt_lemma = sent_splt_lemma[prep_index+1:]\n",
    "            pos_candidate = tag_sent(sent_splt, sent_splt_lemma, query, POS)\n",
    "        \n",
    "        ans_candidate = get_ans_candidate(pos_candidate, [], POS)\n",
    "    \n",
    "#     print ans_candidate\n",
    "    return get_ans_string(sent_splt, ans_candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<type 'list'>, {u'VB': [9, 26, 46], u'JJ': [18, 21, 28, 33, 38], u'NN': [1, 5, 24, 29, 34, 35, 39, 42, 43, 45, 47, 50], u'CD': [11, 15, 27, 32, 49], u'RB': [20], u'IN': [40], u'PO': [30]})\n",
      "[40]\n",
      "None\n",
      "521\n"
     ]
    }
   ],
   "source": [
    "def get_final_result(fname):\n",
    "    counter = float(0)\n",
    "\n",
    "    with open('names.csv', 'a') as csvfile:\n",
    "        fieldnames = ['id', 'answer']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "    csvfile.close()\n",
    "    \n",
    "    with open(fname) as json_data:\n",
    "        infile = json.load(json_data)\n",
    "        length = len(infile)\n",
    "        for dic in infile:  \n",
    "            \n",
    "            #testing\n",
    "            question = dic['question']\n",
    "            docid = dic['docid'] \n",
    "            # question id\n",
    "            qid = dic['id']\n",
    "            \n",
    "            query, q_type = extract_query(question)\n",
    "                        \n",
    "            my_para_ord, my_para = get_para(docid, query)\n",
    "                        \n",
    "            # my_sent is raw sent in the paragraph\n",
    "            my_sent = get_sent(my_para, query)\n",
    "            \n",
    "            my_ans = get_ans(my_sent, query, q_type)\n",
    "            \n",
    "            print qid\n",
    "            \n",
    "            output_one_result({'id': qid, 'answer': my_ans.encode(\"utf-8\")})\n",
    "            \n",
    "develfname = 'project_files/mytest.json' # Need to change later\n",
    "get_final_result(develfname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
